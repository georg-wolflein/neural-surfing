@book{russell2010,
    place={New Jersey},
    edition={3rd},
    title={Artificial Intelligence: A Modern Approach},
    publisher={Pearson},
    author={Russell, Stuart and Norvig, Peter},
    year={2010}
}

@book{burkov2019,
    place={Quebec, Canada},
    title={The Hundred-Page Machine Learning Book},
    publisher={Andriy Burkov},
    author={Burkov, Andriy},
    year={2019}
}

@article{mcculloch1943,
    doi = {10.1007/bf02478259},
    url = {https://doi.org/10.1007/bf02478259},
    year = {1943},
    month = dec,
    publisher = {Springer Science and Business Media {LLC}},
    volume = {5},
    number = {4},
    pages = {115--133},
    author = {Warren S. McCulloch and Walter Pitts},
    title = {A logical calculus of the ideas immanent in nervous activity},
    journal = {The Bulletin of Mathematical Biophysics},
}

@article{rosenblatt1958,
    title={The perceptron: A probabilistic model for information storage and organization in the brain},
    volume={65},
    DOI={10.1037/h0042519},
    number={6},
    journal={Psychological Review},
    author={Rosenblatt, F.},
    year={1958},
    pages={386--408},
    url = {https://doi.org/10.1037/h0042519},
}

@book{hastie2017,
    place={New York},
    edition={2nd},
    title={The Elements of statistical learning: data mining, inference, and prediction},
    publisher={Springer},
    author={Hastie, Trevor and Friedman, Jerome and Tisbshirani, Robert},
    year={2017}
}

@InProceedings{glorot2011,
    title = 	 {Deep Sparse Rectifier Neural Networks},
    author = 	 {Xavier Glorot and Antoine Bordes and Yoshua Bengio},
    booktitle = 	 {Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics},
    pages = 	 {315--323},
    year = 	 {2011},
    editor = 	 {Geoffrey Gordon and David Dunson and Miroslav Dudík},
    volume = 	 {15},
    series = 	 {Proceedings of Machine Learning Research},
    address = 	 {Fort Lauderdale, FL, USA},
    month = 	 {11--13 Apr},
    publisher = 	 {PMLR},
    pdf = 	 {http://proceedings.mlr.press/v15/glorot11a/glorot11a.pdf},
    url = 	 {http://proceedings.mlr.press/v15/glorot11a.html},
    abstract = 	 {While logistic sigmoid neurons are more biologically plausible than hyperbolic tangent neurons, the latter work better for training multi-layer neural networks. This paper shows that rectifying neurons are an even better model of biological neurons and yield equal or better performance than hyperbolic tangent networks in spite of the hard non-linearity and non-differentiability at zero, creating sparse representations with true zeros which seem remarkably suitable for naturally sparse data. Even though they can take advantage of semi-supervised setups with extra-unlabeled data, deep rectifier networks can reach their best performance without requiring any unsupervised pre-training on purely supervised tasks with large labeled datasets. Hence, these results can be seen as a new milestone in the attempts at understanding the difficulty in training deep but purely supervised neural networks, and closing the performance gap between neural networks learnt with and without unsupervised pre-training. [pdf]}
}

@article{neal1992,
    title={Connectionist learning of belief networks},
    volume={56},
    DOI={10.1016/0004-3702(92)90065-6},
    number={1},
    journal={Artificial Intelligence},
    author={Neal, Radford M.},
    year={1992},
    pages={71--113},
    url={https://doi.org/10.1016/0004-3702(92)90065-6}
}

@book{rudin2006,
  title={Functional Analysis},
  author={Rudin, W.},
  isbn={9780070619883},
  lccn={90005677},
  series={International series in pure and applied mathematics},
  year={2006},
  publisher={McGraw-Hill}
}

@article{reem2017,
    title={Remarks on the Cauchy functional equation and variations of it},
    volume={91},
    DOI={10.1007/s00010-016-0463-6},
    number={2},
    journal={Aequationes mathematicae},
    author={Reem, Daniel},
    year={2017},
    month={Jul},
    pages={237--264},
    url={https://doi.org/10.1007/s00010-016-0463-6}
}

@article{blum1989,
    title={Approximation of Boolean Functions by Sigmoidal Networks: Part I: XOR and Other Two-Variable Functions},
    volume={1},
    DOI={10.1162/neco.1989.1.4.532},
    number={4},
    journal={Neural Computation},
    author={Blum, E. K.},
    year={1989},
    pages={532--540},
    url={https://doi.org/10.1162/neco.1989.1.4.532}
}

@article{buhmann2000,
    title={Radial basis functions},
    volume={9},
    DOI={10.1017/S0962492900000015},
    journal={Acta Numerica},
    publisher={Cambridge University Press},
    author={Buhmann, M. D.},
    year={2000},
    pages={1--38},
    url={https://doi.org/10.1017/S0962492900000015}
}

@misc{johnson2015,
    Author = {Steven G. Johnson},
    Title = {Saddle-point integration of {$C_\infty$} ``bump'' functions},
    Year = {2015},
    Eprint = {arXiv:1508.04376},
    url={https://arxiv.org/abs/1508.04376}
}

@article{lecun2015,
    doi = {10.1038/nature14539},
    url = {https://doi.org/10.1038/nature14539},
    year = {2015},
    month = may,
    publisher = {Springer Science and Business Media {LLC}},
    volume = {521},
    number = {7553},
    pages = {436--444},
    author = {Yann LeCun and Yoshua Bengio and Geoffrey Hinton},
    title = {Deep learning},
    journal = {Nature}
}

@misc{goldblum2019,
    Author = {Micah Goldblum and Jonas Geiping and Avi Schwarzschild and Michael Moeller and Tom Goldstein},
    Title = {Truth or Backpropaganda? An Empirical Investigation of Deep Learning Theory},
    Year = {2019},
    Eprint = {arXiv:1910.00359},
    url = {https://arxiv.org/abs/1809.10749}
}

@misc{nguyen2018,
    Author = {Quynh Nguyen and Mahesh Chandra Mukkamala and Matthias Hein},
    Title = {On the loss landscape of a class of deep neural networks with no bad local valleys},
    Year = {2018},
    Eprint = {arXiv:1809.10749},
    url={https://arxiv.org/abs/1809.10749}
}

@InProceedings{laurent2018,
    title = 	 {Deep Linear Networks with Arbitrary Loss: All Local Minima Are Global},
    author = 	 {Laurent, Thomas and von Brecht, James},
    booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
    pages = 	 {2902--2907},
    year = 	 {2018},
    editor = 	 {Dy, Jennifer and Krause, Andreas},
    volume = 	 {80},
    series = 	 {Proceedings of Machine Learning Research},
    address = 	 {Stockholmsmässan, Stockholm Sweden},
    month = 	 {10--15 Jul},
    publisher = 	 {PMLR},
    pdf = 	 {http://proceedings.mlr.press/v80/laurent18a/laurent18a.pdf},
    url = 	 {http://proceedings.mlr.press/v80/laurent18a.html}
}

@misc{kawaguchi2016,
    Author = {Kenji Kawaguchi},
    Title = {Deep Learning without Poor Local Minima},
    Year = {2016},
    Eprint = {arXiv:1605.07110},
    url={https://arxiv.org/abs/1605.07110}
}

@misc{weir2019,
    place={St Andrews},
    title={{CS3105}: Cases \& Approaches 2: Neural Local Minima},
    url={https://studres.cs.st-andrews.ac.uk/2018_2019/CS3105/Lectures/CS3105%20Cases%20&%20Approaches/CS3105%20Case%20&%20Approach%202%20S2_19.pdf},
    journal={Lecture},
    author={Weir, Michael K.},
    year={2019},
    month={Apr}
}

@book{loomis1990,
  title={Advanced Calculus},
  author={Loomis, L.H. and Sternberg, S.},
  isbn={9780867201222},
  lccn={lc68012226},
  series={Math Series},
  year={1990},
  publisher={Jones and Bartlett Publishers}
}

@article{rios2009,
    author = {Rios, Luis and Sahinidis, Nikolaos},
    year = {2009},
    month = {11},
    pages = {},
    title = {Derivative-free optimization: A review of algorithms and comparison of software implementations},
    volume = {56},
    journal = {Journal of Global Optimization},
    doi = {10.1007/s10898-012-9951-y},
    url = {https://doi.org/10.1007/s10898-012-9951-y},
}

@article {kirkpatrick1983,
	author = {Kirkpatrick, S. and Gelatt, C. D. and Vecchi, M. P.},
	title = {Optimization by Simulated Annealing},
	volume = {220},
	number = {4598},
	pages = {671--680},
	year = {1983},
	doi = {10.1126/science.220.4598.671},
	publisher = {American Association for the Advancement of Science},
	abstract = {There is a deep and useful connection between statistical mechanics (the behavior of systems with many degrees of freedom in thermal equilibrium at a finite temperature) and multivariate or combinatorial optimization (finding the minimum of a given function depending on many parameters). A detailed analogy with annealing in solids provides a framework for optimization of the properties of very large and complex systems. This connection to statistical mechanics exposes new information and provides an unfamiliar perspective on traditional optimization problems and methods.},
	issn = {0036-8075},
	URL = {https://science.sciencemag.org/content/220/4598/671},
	eprint = {https://science.sciencemag.org/content/220/4598/671.full.pdf},
	journal = {Science}
}

@article{cerny1985,
    doi = {10.1007/bf00940812},
    url = {https://doi.org/10.1007/bf00940812},
    year = {1985},
    month = jan,
    publisher = {Springer Science and Business Media {LLC}},
    volume = {45},
    number = {1},
    pages = {41--51},
    author = {V. {\v{C}}ern{\'{y}}},
    title = {Thermodynamical approach to the traveling salesman problem: An efficient simulation algorithm},
    journal = {Journal of Optimization Theory and Applications}
}

@article{belisle1993,
    ISSN = {0364765X, 15265471},
    URL = {http://www.jstor.org/stable/3690278},
    abstract = {We introduce a general class of Hit-and-Run algorithms for generating essentially arbitrary absolutely continuous distributions on Rd. They include the Hypersphere Directions algorithm and the Coordinate Directions algorithm that have been proposed for identifying nonredundant linear constraints and for generating uniform distributions over subsets of Rd. Given a bounded open set S in Rd, an absolutely continuous probability distribution π on S (the target distribution) and an arbitrary probability distribution ν on the boundary of the d-dimensional unit sphere centered at the origin (the direction distribution), the (ν, π)-Hit-and-Run algorithm produces a sequence of iteration points as follows. Given the nth iteration point x, choose a direction θ according to the distribution ν and then choose the (n + 1)st iteration point according to the conditionalization of the distribution π along the line defined by x and x + θ. Under mild conditions, we show that this sequence of points is a Harris recurrent reversible Markov chain converging in total variation to the target distribution π.},
    author = {Claude J. P. Bélisle and H. Edwin Romeijn and Robert L. Smith},
    journal = {Mathematics of Operations Research},
    number = {2},
    pages = {255--266},
    publisher = {INFORMS},
    title = {Hit-and-Run Algorithms for Generating Multivariate Distributions},
    volume = {18},
    year = {1993}
}

@InProceedings{aly2019,
    author={A. {Aly} and G. {Guadagni} and J. B. {Dugan}},
    booktitle={2019 {IEEE} 10th Annual Ubiquitous Computing, Electronics Mobile Communication Conference (UEMCON)},
    title={Derivative-Free Optimization of Neural Networks using Local Search},
    year={2019},
    month={Oct.},
    pages={293--299},
    url = {https://doi.org/10.1109/UEMCON47517.2019.8993007},
    doi = {10.1109/UEMCON47517.2019.8993007}
}

@ARTICLE{battiti1995,
    author={R. {Battiti} and G. {Tecchiolli}},
    journal={{IEEE} Transactions on Neural Networks},
    title={Training neural nets with the reactive tabu search},
    year={1995},
    month={Sep.},
    volume={6},
    number={5},
    pages={1185-1200},
    doi={10.1109/72.410361},
    url={https://doi.org/10.1109/72.410361}
}

@InProceedings{hirasawa1998,
    author={K. {Hirasawa} and K. {Togo} and J. {Murata} and M. {Ohbayashi} and N. {Shao} and J. {Hu}},
    booktitle={1998 {IEEE} International Joint Conference on Neural Networks Proceedings. {IEEE} World Congress on Computational Intelligence (Cat. No.98CH36227)},
    title={A new random search method for neural networks learning-random search with variable search length (RasVal)},
    year={1998},
    volume={2},
    pages={1602-1607 vol.2},
    doi={10.1109/IJCNN.1998.686017},
    url={https://doi.org/10.1109/72.410361},
    month={May}
}

@article{such2017,
    author    = {Felipe Petroski Such and
                Vashisht Madhavan and
                Edoardo Conti and
                Joel Lehman and
                Kenneth O. Stanley and
                Jeff Clune},
    title     = {Deep Neuroevolution: Genetic Algorithms Are a Competitive Alternative
                for Training Deep Neural Networks for Reinforcement Learning},
    journal   = {CoRR},
    volume    = {abs/1712.06567},
    year      = {2017},
    url       = {http://arxiv.org/abs/1712.06567},
    archivePrefix = {arXiv},
    eprint    = {1712.06567},
    timestamp = {Mon, 13 Aug 2018 16:46:09 +0200},
}

@book{press1992,
    author = {Press, William H. and Teukolsky, Saul A. and Vetterling, William T. and Flannery, Brian P.},
    title = {Numerical Recipes in C (2nd Ed.): The Art of Scientific Computing},
    year = {1992},
    isbn = {0521431085},
    publisher = {Cambridge University Press},
    address = {USA}
}

@article{nelder1965,
    author = {Nelder, J. A. and Mead, R.},
    title = "{A Simplex Method for Function Minimization}",
    journal = {The Computer Journal},
    volume = {7},
    number = {4},
    pages = {308-313},
    year = {1965},
    month = {01},
    abstract = "{A method is described for the minimization of a function of n variables, which depends on the comparison of function values at the (n + 1) vertices of a general simplex, followed by the replacement of the vertex with the highest value by another point. The simplex adapts itself to the local landscape, and contracts on to the final minimum. The method is shown to be effective and computationally compact. A procedure is given for the estimation of the Hessian matrix in the neighbourhood of the minimum, needed in statistical estimation problems.}",
    issn = {0010-4620},
    doi = {10.1093/comjnl/7.4.308},
    url = {https://doi.org/10.1093/comjnl/7.4.308},
    eprint = {https://academic.oup.com/comjnl/article-pdf/7/4/308/1013182/7-4-308.pdf},
}

@inproceedings{lewis1999,
    author={Lewis, Jonathan P. and Weir, Michael K.},
    booktitle={{IJCNN'99.} International Joint Conference on Neural Networks. Proceedings (Cat. No.99CH36339)},
    title={Subgoal chaining and the local minimum problem},
    year={1999},
    volume={3}, 
    pages={1844-1849},
    doi={10.1109/IJCNN.1999.832660},
    ISSN={1098-7576},
    month={July}
}

@inproceedings{weir2000,
    title={Using Tangent Hyperplanes to Direct Neural Training},
    author={Weir, Michael K. and Lewis, Jonathan P. and Milligan, G.},
    booktitle={Proceedings of the International {ICSC} Symposium on Neural Computation},
    year={2000}
}

@article{gorse1997,
    title = "The New {ERA} in Supervised Learning",
    journal = "Neural Networks",
    volume = "10",
    number = "2",
    pages = "343 - 352",
    year = "1997",
    issn = "0893-6080",
    doi = "https://doi.org/10.1016/S0893-6080(96)00090-1",
    url = "http://www.sciencedirect.com/science/article/pii/S0893608096000901",
    author = "Denise Gorse and Adrian J. Shepherd and John G. Taylor",
    keywords = "Global optimisation, Local minima, Homotopy, Range expansion",
    abstract = "Conventional methods of supervised learning are inevitably faced with the problem of local minima; evidence is presented that second order methods such as the conjugate gradient and quasi-Newton techniques are particularly susceptible to being trapped in sub-optimal solutions. A new technique, expanded range approximation (ERA), is presented, which by the use of a homotopy on the range of the target outputs allows supervised learning methods to find a global minimum of the error function in almost every case. © 1997 Elsevier Science Ltd. All Rights Reserved."
}

@article{rumelhart1986,
    doi = {10.1038/323533a0},
    url = {https://doi.org/10.1038/323533a0},
    year = {1986},
    month = oct,
    publisher = {Springer Science and Business Media {LLC}},
    volume = {323},
    number = {6088},
    pages = {533--536},
    author = {David E. Rumelhart and Geoffrey E. Hinton and Ronald J. Williams},
    title = {Learning representations by back-propagating errors},
    journal = {Nature}
}

@misc{kingma2014,
    Author = {Diederik P. Kingma and Jimmy Ba},
    Title = {Adam: A Method for Stochastic Optimization},
    Year = {2014},
    Eprint = {arXiv:1412.6980},
    url = {https://arxiv.org/abs/1412.6980}
}

@inproceedings{karayiannis1998,
    author = {Karayiannis, N.B.},
    year = {1998},
    month = {06},
    pages = {2230 - 2235 vol.3},
    title = {Learning algorithms for reformulated radial basis neural networks},
    volume = {3},
    isbn = {0-7803-4859-1},
    doi = {10.1109/IJCNN.1998.687207}
}

@article{friedman1977,
    author = {Friedman, Jerome H. and Bentley, Jon Louis and Finkel, Raphael Ari},
    title = {An Algorithm for Finding Best Matches in Logarithmic Expected Time},
    year = {1977},
    issue_date = {September 1977},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {3},
    number = {3},
    issn = {0098-3500},
    url = {https://doi.org/10.1145/355744.355745},
    doi = {10.1145/355744.355745},
    journal = {ACM Trans. Math. Softw.},
    month = sep,
    pages = {209--226},
    numpages = {18}
}

@online{github2019,
    title={The State of the Octoverse},
    url={https://octoverse.github.com/},
    publisher={GitHub Inc.},
    author={GitHub},
    year={2019}
}

@online{elliott2019,
    title={The State of the Octoverse},
    url={https://github.blog/2019-01-24-the-state-of-the-octoverse-machine-learning/},
    publisher={GitHub Inc.},
    author={Elliott, Thomas},
    year={2019},
    month=jan,
    journal={The GitHub Blog}
}

@misc{tensorflow2015whitepaper,
    title={ {TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
    url={https://www.tensorflow.org/},
    author={
        Mart\'{i}n~Abadi and
        Ashish~Agarwal and
        Paul~Barham and
        Eugene~Brevdo and
        Zhifeng~Chen and
        Craig~Citro and
        Greg~S.~Corrado and
        Andy~Davis and
        Jeffrey~Dean and
        Matthieu~Devin and
        Sanjay~Ghemawat and
        Ian~Goodfellow and
        Andrew~Harp and
        Geoffrey~Irving and
        Michael~Isard and
        Yangqing Jia and
        Rafal~Jozefowicz and
        Lukasz~Kaiser and
        Manjunath~Kudlur and
        Josh~Levenberg and
        Dandelion~Man\'{e} and
        Rajat~Monga and
        Sherry~Moore and
        Derek~Murray and
        Chris~Olah and
        Mike~Schuster and
        Jonathon~Shlens and
        Benoit~Steiner and
        Ilya~Sutskever and
        Kunal~Talwar and
        Paul~Tucker and
        Vincent~Vanhoucke and
        Vijay~Vasudevan and
        Fernanda~Vi\'{e}gas and
        Oriol~Vinyals and
        Pete~Warden and
        Martin~Wattenberg and
        Martin~Wicke and
        Yuan~Yu and
        Xiaoqiang~Zheng},
    year={2015}
}

@article{choi2008,
    doi = {10.1016/j.neucom.2008.04.004},
    url = {https://doi.org/10.1016/j.neucom.2008.04.004},
    year = {2008},
    month = oct,
    publisher = {Elsevier {BV}},
    volume = {71},
    number = {16-18},
    pages = {3640--3643},
    author = {Bumghi Choi and Ju-Hong Lee and Deok-Hwan Kim},
    title = {Solving local minima problem with large number of hidden nodes on two-layered feed-forward artificial neural networks},
    journal = {Neurocomputing}
}