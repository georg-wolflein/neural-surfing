\begin{titlepage}
	\centering
	
	{\scshape\LARGE Senior Honours Project\par}
	\vspace{0.25cm}
	{\includegraphics[width=0.8\textwidth]{logo.png} \par}
	\vspace{0.25cm}
	{\huge\bfseries Freeing Neural Training\\Through Surfing\par}
	\vspace{0.5cm}

	\vfill

	\noindent
	\begin{minipage}{0.45\textwidth}
		\begin{center} \large
		  \textit{Author:}\\
          Georg \textsc{Wölflein}\\
		\end{center}
    \end{minipage}%
    \begin{minipage}{0.45\textwidth}
		\begin{center} \large
		\textit{Supervisor:} \\
		Dr.~Michael \textsc{Weir}
		\end{center}
	\end{minipage}%

	% \vspace{0.5cm}
	\vfill

	{April 27, 2020\par}
\end{titlepage}

\chapter*{Abstract}
Gradient methods based on backpropagation are widely used in training multilayer feedforward neural networks. 
However, such algorithms often converge to suboptimal weight configurations known as local minima. 
This report presents a novel minimal example of the local minimum problem with only three training samples and demonstrates its suitability for investigating and resolving said problem by analysing its mathematical properties and conditions leading to the failure of conventional training regimes. 
A different perspective for training neural networks is introduced that concerns itself with neural spaces and is applied to study the local minimum example.
This gives rise to the concept of setting intermediate subgoals during training which is demonstrated to be a viable and effective means of overcoming the local minimum problem. 
The versatility of subgoal-based approaches is highlighted by showing their potential for training more generally. 
An example of a subgoal-based training regime using sampling and an adaptive clothoid for establishing a goal-connecting path is suggested as a proof of concept for further research. 
In addition, this project includes the design and implementation of a software framework for monitoring the performance of different neural training algorithms on a given problem simultaneously and in real time. 
This framework can be used to reproduce the findings of how classical algorithms fail to find the global minimum in the aforementioned example.

\begin{otherlanguage}{ngerman}
\chapter*{Zusammenfassung}
Auf Fehlerrückführung basierende Gradientenmethoden sind für das Training vorwärts gekoppelter mehrschichtiger neuronaler Netze weit verbreitet.
Solche Algorithmen konvergieren jedoch häufig zu suboptimalen Gewichtsbelegungen, die als lokale Minima bezeichnet werden.
Diese Arbeit präsentiert ein neuartiges Minimalbeispiel des lokalen Minimumproblems mit nur drei Trainingsmustern und belegt dessen Eignung zur Erforschung und Lösung dieses Problems durch die Analyse dessen mathematischer Eigenschaften sowie der Bedingungen, die zum Scheitern herkömmlicher Trainingsregimes führen.
Es wird eine neue Sichtweise auf das Training neuronaler Netze erarbeitet, die sich vor allem mit den Zusammenhängen neuronaler Räume befasst und anschließend zur Untersuchung des o.g. Problems angewandt wird.
Daraus ergibt sich der Ansatz, während des Trainings dynamische Zwischenziele festzulegen, was sich als praktikables und wirksames Mittel zur Überwindung des lokalen Minimumproblems erweist.
Das Potenzial solcher auf Zwischenzielen basierender Verfahren wird durch die theoretische Untersuchung der Anwendbarkeit auf andere Trainingsprobleme und -systeme aufgezeigt. 
Als Machbarkeitsnachweis für weitere Forschungsarbeiten wird ein Beispiel eines solchen Verfahrens vorgeschlagen, bei dem die kontinuierliche Auswertung von Stichproben in zwei neuronalen Räumen zur Entwicklung einer sich adaptiv verformenden Zielverlaufskurve in Form einer Klothoide führt.
Darüber hinaus umfasst diese Arbeit die Planung und Umsetzung eines Software-Frameworks, welches vergleichend die Leistung verschiedener neuronaler Algorithmen für vorgegebene Probleme während des Trainings in Echtzeit abbildet.
Dieses Framework erlaubt u.a. die Nachbildung des Ergebnisses, dass klassische Algorithmen im o.g. Beispiel des lokalen Minimumproblems nicht in der Lage sind, das globale Minimum zu finden.
\end{otherlanguage}

\chapter*{Declaration}
I declare that the material submitted for assessment is my own work except where credit is explicitly given to others by citation or acknowledgement.
This work was performed during the current academic year except where otherwise stated.

The main text of this project report is \quickwordcount{report} long.

In submitting this project report to the University of St Andrews, I give permission for it to be made available for use in accordance with the regulations of the University Library. 
I also give permission for the title and abstract to be published and for copies of the report to be made and supplied at cost to any bona fide library or research worker, and to be made available on the World Wide Web.
I retain the copyright in this work.

\vspace{0.5cm}

\textit{Georg Wölflein}

\chapter*{Acknowledgements}
I would like to express my gratitude to my supervisor Dr.~Michael Weir for his valuable guidance and constructive suggestions throughout the course of this project. 
His willingness to devote time in explaining issues and analysing problems has been much appreciated, and I would like to thank him especially for the detailed written correspondence in the time of the Coronavirus.
I would also like to thank Zahida Almuallem for her valuable insights and suggestions during this project.
I am very grateful to the School of Computer Science and University of St Andrews for an exceptional undergraduate education and the swift transition to online teaching during the Coronavirus crisis.
Finally, I wish to thank my parents for providing me with the opportunities and encouragement to pursue higher education.

\tableofcontents
\listoffigures

\chapter*{Notation}
\begin{table}[h!]
    \centering
    \begin{tabular}{l|l}
        \hline
        Notation & Description \\
        \hline
        $N$ & Number of training samples \\
        $D$ & Number of input features \\
        $x_i$ & The $i$th input feature ($1 \leq i \leq D$) \\
        $\vec{x}_i$ & Input feature vector of the $i$th training sample ($1 \leq i \leq N$) \\
        $\vec{X}$ & Input matrix ($N \times D$) \\
        $y_i$ & Output target for the $i$th training sample ($1 \leq i \leq N$) \\
        $\hat{y}_i$ & Output prediction of the $i$th training sample ($1 \leq i \leq N$) \\
        $\vec{y}$ & Output target vector ($N$-dimensional) \\
        $\vec{\hat{y}}$ & Output prediction vector ($N$-dimensional) \\
        $w_i$ & The $i$th weight \\
        $\vec{w}$ & Weight vector \\
        $\vec{W}$ & Weight matrix \\
        $b$ & Bias term \\
        $\vec{b}$ & Bias vector \\
        $\mathscr{P}$ & Trainable parameters (weights and biases) \\
        $g(\cdot)$ & Activation function \\
        $\phi(\cdot)$ & Radial basis function \\
        $R(\cdot)$ & Regression model \\
        $S(\cdot)$ & Single-layer network with one output \\
        $\vec{S}(\cdot)$ & Single-layer network with multiple outputs \\
        $M(\cdot)$ & Multi-layer perceptron \\
        $\mathcal{I}$ & Input space \\
        $\mathcal{W}$ & Weight space \\
        $\mathcal{O}$ & Output space \\
        $\mathcal{O}(\cdot)$ & Big O notation \\
        $\vec{J}$ & Jacobian \\
        $\vec{H}$ & Hessian \\
        $\norm{\cdot}$ & Euclidean (L2) norm \\
        $\measuredangle$ & Angle \\
        $\triangle$ & Triangle \\
        $\mathbb{R}$ & The set of real numbers
    \end{tabular}
\end{table}

\noindent
Scalar values are generally denoted $v$, vectors $\vec{v}$, and matrices $\vec{V}$ (their inverse is $\vec{V}^{-1}$ and transpose is $\vec{V}\tran$).
Vector-valued functions are denoted in bold font.