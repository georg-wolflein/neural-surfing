\begin{titlepage}
	\centering
	
	{\scshape\LARGE Senior Honours Project\par}
	\vspace{0.25cm}
	{\includegraphics[width=0.8\textwidth]{logo.png} \par}
	\vspace{0.25cm}
	{\huge\bfseries Freeing Neural Training Through Surfing\par}
	\vspace{0.5cm}

	\vfill

	\noindent
	\begin{minipage}{0.45\textwidth}
		\begin{center} \large
		  \textit{Author:}\\
          Georg \textsc{Wölflein}\\
		\end{center}
    \end{minipage}%
    \begin{minipage}{0.45\textwidth}
		\begin{center} \large
		\textit{Supervisor:} \\
		Dr.~Michael \textsc{Weir}
		\end{center}
	\end{minipage}%

	% \vspace{0.5cm}
	\vfill

	{April 27, 2020\par}
\end{titlepage}

\chapter*{Abstract}
Gradient methods based on backpropagation are widely used in training multilayer feedforward neural networks. 
However, such algorithms often converge to suboptimal weight configurations known as local minima. 
This report presents a novel minimal example of the local minimum problem with only three training samples and demonstrates its suitability for investigating and resolving said problem by analysing its mathematical properties and conditions leading to the failure of conventional training regimes. 
A different perspective for training neural networks is introduced that concerns itself with neural spaces and is applied to study the local minimum example.
This gives rise to the concept of setting intermediate subgoals during training which is demonstrated to be a viable and effective means of overcoming the local minimum problem. 
The versatility of subgoal-based approaches is highlighted by showing their potential for training more generally. 
An example of a subgoal-based training regime using sampling and an adaptive clothoid for establishing a goal-connecting path is suggested as a proof of concept for further research. 
In addition, this project includes the design and implementation of a software framework for monitoring the performance of different neural training algorithms on a given problem simultaneously and in real time. 
This framework can be used to reproduce the findings of how classical algorithms fail to find the global minimum in the aforementioned example.


\chapter*{Declaration}
I declare that the material submitted for assessment is my own work except where credit is explicitly given to others by citation or acknowledgement.
This work was performed during the current academic year except where otherwise stated.

The main text of this project report is \quickwordcount{report} long.

In submitting this project report to the University of St Andrews, I give permission for it to be made available for use in accordance with the regulations of the University Library. 
I also give permission for the title and abstract to be published and for copies of the report to be made and supplied at cost to any bona fide library or research worker, and to be made available on the World Wide Web.
I retain the copyright in this work.

\vspace{0.5cm}

\textit{Georg Wölflein}

\chapter*{Acknowledgements}
I would like to express my gratitude to my supervisor Dr.~Michael Weir for his valuable guidance and constructive suggestions throughout the course of this project. 
His willingness to devote time in explaining issues and analysing problems has been much appreciated, and I would like to thank him especially for the detailed written correspondence in the time of the Coronavirus.
I would also like to thank Zahida Almuallem for her valuable insights and suggestions during this project.
I am very grateful to the School of Computer Science and University of St Andrews for an exceptional undergraduate education and the swift transition to online teaching during the Coronavirus crisis.
Finally, I wish to thank my parents for providing me with the opportunities and encouragement to pursue higher education.

\tableofcontents
\listoffigures

\chapter*{Notation}
\begin{table}[h!]
    \centering
    \begin{tabular}{l|l}
        \hline
        Notation & Description \\
        \hline
        $N$ & Number of training samples \\
        $D$ & Number of input features \\
        $x_i$ & The $i$th input feature ($1 \leq i \leq D$) \\
        $\vec{x}_i$ & Input feature vector of the $i$th training sample ($1 \leq i \leq N$) \\
        $\vec{X}$ & Input matrix ($N \times D$) \\
        $y_i$ & Output target for the $i$th training sample ($1 \leq i \leq N$) \\
        $\hat{y}_i$ & Output prediction of the $i$th training sample ($1 \leq i \leq N$) \\
        $\vec{y}$ & Output target vector ($N$-dimensional) \\
        $\vec{\hat{y}}$ & Output prediction vector ($N$-dimensional) \\
        $w_i$ & The $i$th weight \\
        $\vec{w}$ & Weight vector \\
        $\vec{W}$ & Weight matrix \\
        $b$ & Bias term \\
        $\vec{b}$ & Bias vector \\
        $\mathscr{P}$ & Trainable parameters (weights and biases) \\
        $g(\cdot)$ & Activation function \\
        $\phi(\cdot)$ & Radial basis function \\
        $R(\cdot)$ & Regression model \\
        $S(\cdot)$ & Single-layer network with one output \\
        $\vec{S}(\cdot)$ & Single-layer network with multiple outputs \\
        $M(\cdot)$ & Multi-layer perceptron \\
        $\mathcal{I}$ & Input space \\
        $\mathcal{W}$ & Weight space \\
        $\mathcal{O}$ & Output space \\
        $\mathcal{O}(\cdot)$ & Big O notation \\
        $\vec{J}$ & Jacobian \\
        $\vec{H}$ & Hessian \\
        $\norm{\cdot}$ & Euclidean (L2) norm \\
        $\measuredangle$ & Angle \\
        $\triangle$ & Triangle \\
        $\mathbb{R}$ & The set of real numbers \\
        $\mathbb{B}$ & The Boolean domain ($\{0,1\}$) \\
        $\neg$ & Logical NOT \\
        $\xor$ & Logical XOR
    \end{tabular}
\end{table}

\noindent
Scalar values are generally denoted $v$, vectors $\vec{v}$, and matrices $\vec{V}$ (their inverse is $\vec{V}^{-1}$ and transpose is $\vec{V}\tran$).
Vector-valued functions are denoted in bold font.