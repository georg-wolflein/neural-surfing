\chapter{Introduction}
\textit{Describe the problem you set out to solve and the
extent of your success in solving it. You should include
the aims and objectives of the project in order of
importance and try to outline key aspects of your
project for the reader to look for in the rest of your
report.}
\todo

\chapter{Context survey}
\section{Neural networks}

\paragraph{Backpropagation}
\todo
(BP)

\paragraph{Derivative-free optimisation}
The class of derivative-free optimisation (DFO) algorithms are optimisation techniques that attempt to find a global optimum, requiring only an objective function, but no gradient information.
One example of such an algorithm is simulated annealing (SA), proposed by \citeauthor{kirkpatrick1983}, that mimics the motion of atoms in the physical process of a slowly cooling material \cite*{kirkpatrick1983}.
Originally employed in discrete optimisation problems such as the combinatorial travelling salesman problem \cite{cerny1985}, it was later generalized and applied to problems with continuous domains \cite{belisle1993}.
However, in a comparative study of derivative-free optimisation algorithms, \citeauthor{rios2009} found that SA performed relatively poorly in comparison to more modern DFOs on general optimisation problems\footnote{It is important to note that \citeauthor{rios2009} did not assess DFOs for the purpose of neural network optimisation, but rather compared their performances on general convex and non-convex optimisation problems.} \cite*{rios2009}.

The concept of applying DFO as a means of training neural networks is not unique to this project.
In the 1990s, several training regimes for neural networks were proposed that did not rely on derivative calculations, employing variants of random and local search \cite{hirasawa1998,battiti1995}.
These approaches seemed to find better minima and did not get stuck in local minima as BP did. 
More recently, random search was affirmed in outperforming BP in the context of deep neural networks for reinforcement learning, although a different family of DFO algorithms, so-called genetic algorithms were proposed as a superior alternative \cite{such2017}.

The most recent and most similar work is a DFO technique for neural networks using a variant of local search belonging in the family of random search algorithms \cite{aly2019}.
This technique parallels the finding from other works that DFOs are often able to escape local minima and thus produce better training results; however, they require more iterations and computational resources than BP. 

\citeauthor{aly2019}, \citeauthor{such2017}, and similar works studied the performance of their respective DFO algorithms for training neural networks with a large parameter space (in the order of $10^6$ parameters) which, while providing valuable practical insight, made it impossible to examine the structure of the loss surface analytically in order to assess issues such as severely suboptimal local minima.

\paragraph{The local minimum problem}
The local minimum problem that arises when neural network training converges to a suboptimal local minimum with a comparatively high loss value, has been extensively studied as a phenomenon in optimisation problems.
However, with regards to neural networks, there seems to be different opinions on the severity of this issue. 
One frequently cited article claims that ``In practice, poor local minima are rarely a problem with large networks'' \cite{lecun2015}.
This is underpinned in theory by other works which proved the nonexistence of suboptimal local minima, although they make varying assumptions on the structure of the underlying neural networks \cite{kawaguchi2016,nguyen2018,laurent2018}.
On the other hand, a recent article asserts that ``The apparent scarcity of poor local minima has lead practitioners to develop the intuition that bad local minima \elide are practically non-existent'' \cite{goldblum2019}.
Therefore, it can be said that the local minimum problem is still an active area of research.

\section{Implementation tools}
\begin{itemize}
    \item TensorFlow
    \item keras
\end{itemize}
\todo

\chapter{Requirements specification}
Primary objectives:
\begin{enumerate}
    \item Design a generic framework that can be used for various neural training algorithms
    with a clear set of inputs and outputs at each step. This framework should include
    benchmarking capabilities.
    \item For a simple case of this framework (when the dimensionality of the control space
    and output space are suitably low), implement a visualisation tool that shows the
    algorithmâ€™s steps.
    \item Design and implement the neural surfing technique.
    \item Evaluate the performance of this and other algorithms on tasks of differing
    complexity, especially with regard to the local minimum problem and similar issues.
\end{enumerate}
Secondary objectives:
\begin{enumerate}
    \item Investigate how this approach can be generalized to other numerical optimisation problems.
\end{enumerate}

\section{Ethics}
There are no ethical considerations. 
All questions on the preliminary self-assessment form were answered with ``NO'' and hence no ethics form had to be completed.
