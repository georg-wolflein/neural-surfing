\chapter{Introduction}

This project is primarily a research project; its main focus lies in contriving and investigating an example of the local minimum problem in neural networks\footnote{Throughout this report, the terms \textit{neural network} and \textit{artifical neural network} (ANN) are used interchangeably.} and designing a training regime that tackles this issue. 
Of auxiliary importance is a software framework that should be developed to not only to compare different neural training techniques, but also to ensure that the main theoretical results obtained regarding the local minimum problem are empirically verified and reproducible.

This report begins with a context survey identifying not only the classical neural training algorithms, but also the efforts that have been made to deal with the local minimum problem and their limitations.
The classical theory of neural networks is presented from the ground up in \ref{chap:neural_network_theory} which naturally leads to the analysis of popular neural training techniques in \ref{chap:neural_training}.
In \ref{chap:neural_surfing_theory}, a different paradigm of viewing neural training -- `neural surfing theory' -- is introduced.
\ref{chap:local_minimum_problem} provides a precise definition of the local minimum problem, thereafter facilitating the design of an instance of said problem which is then analysed from both viewpoints described in \ref{chap:neural_network_theory,chap:neural_surfing_theory}.
The subsequent chapter lays out the neural surfing technique for undertaking the local minimum problem.
Finally, an outlook is provided how this type of technique can be generalised to other problems before finally presenting the software framework and an evaluation of the project.

\section{Context survey}
\subsection{Neural training}
\label{sec:context_anns}
The first mathematical model representing neurons in the human brain, so-called \textit{perceptons}, was formulated by \textcite{mcculloch1943} (see Section \ref{sec:ann}).
In 1958, the psychologist Frank Rosenblatt published the first percepton learning algorithm \cite{rosenblatt1958}, but this type of network lacked the ability to learn mappings that were not \textit{linearly separable}.
It was not until the 1980s with the introduction of the backpropagation algorithm capable of training networks with hidden layers, that neural networks experienced a substantial rise in popularity.

\paragraph{Backpropagation}
To this date, the backpropagation (BP) algorithm, attributed to \textcite{rumelhart1986}, remains the prominent method of training neural networks.
It involves computing the gradient of the loss function with respect to the weights and then using some gradient-based optimisation technique such as gradient descent to update the weights.
With the rise in popularity of deep neural networks, methods have been developed to increase the speed of converging to a minimum. 
The two main approaches are parallelising the computation and using adaptive learning rates like in the `Adam optimizer' \cite{kingma2014}.
It is well-established that BP, provided a suitable choice of hyperparameters, is guaranteed to converge to a local (but likely not global) minimum.
A common technique to subdue the effect of this issue is to run BP multiple times with different random initialisations.

\paragraph{Derivative-free optimisation}
The class of derivative-free optimisation (DFO) algorithms are optimisation techniques that attempt to find a global optimum, requiring only an objective function, but no gradient information.
One example of such an algorithm is simulated annealing (SA), proposed by \citeauthor{kirkpatrick1983}, that mimics the motion of atoms in the physical process of a slowly cooling material \cite*{kirkpatrick1983}.
Originally employed in discrete optimisation problems such as the combinatorial travelling salesman problem \cite{cerny1985}, it was later generalised and applied to problems with continuous domains \cite{belisle1993}.
However, in a comparative study of derivative-free optimisation algorithms, \citeauthor{rios2009} found that SA performed relatively poorly in comparison to more modern DFOs on general optimisation problems\footnote{It is important to note that \citeauthor{rios2009} did not assess DFOs for the purpose of neural network optimisation, but rather compared their performances on general convex and non-convex optimisation problems.} \cite*{rios2009}.

The concept of applying DFO as a means of training neural networks is not unique to this project.
In the 1990s, several training regimes for neural networks were proposed that did not rely on derivative calculations, employing variants of random and local search \cite{hirasawa1998,battiti1995}.
These approaches seemed to find better minima in some settings and did not get stuck in local minima as BP did. 
More recently, a particular random search approach was affirmed in outperforming BP in the context of deep neural networks for reinforcement learning, although a different family of DFO algorithms, so-called genetic algorithms were proposed as a superior alternative \cite{such2017}.

A very recent work presents a DFO technique for neural networks that uses a variant of local search belonging in the family of random search algorithms \cite{aly2019}.
This technique parallels the finding from other works that DFOs are often able to escape some\footnote{Guaranteed convergence to a global minimum in every scenario is not asserted, although the results indicate that the local minima are not as `poor'.} local minima and thus produce better training results; however, they require more iterations and computational resources than BP. 

\citeauthor{aly2019}, \citeauthor{such2017}, and similar works studied the performance of their respective DFO algorithms for training neural networks with a large parameter space (in the order of $10^6$ parameters) which, while providing valuable practical insight, made it infeasible to examine the structure of the loss surface analytically in order to assess issues such as severely suboptimal local minima.

\subsection{The local minimum problem}
\label{sec:context_local_minimum_problem}
The local minimum problem, which arises when an algorithm converges to a suboptimal local minimum with a comparatively high loss value, has been extensively studied as a phenomenon in optimisation problems.
However, with regards to neural networks, there seems to be differing opinions on the severity of this issue. 
One frequently cited article claims that ``In practice, poor local minima are rarely a problem with large networks'' \cite{lecun2015}.
This is underpinned in theory by other works which proved the nonexistence of suboptimal local minima, although they make varying assumptions on the structure of the underlying neural networks \cite{kawaguchi2016,nguyen2018,laurent2018}.
On the other hand, a recent article asserts that ``The apparent scarcity of poor local minima has lead practitioners to develop the intuition that bad local minima \elide are practically non-existent'' \cite{goldblum2019}.
Notwithstanding this apparent disagreement, the neural local minimum problem remains an active area of research.

There have been various approaches attempting to overcome the local minimum problem as it relates to neural training.
\citeauthor{choi2008} presented a method whereby the network is split into two separate parts that are trained separately, but this technique works only on networks with one hidden layer \cite*{choi2008}.
\citeauthor{lo2012} followed a different approach through which the mean squared error function is modified in order to `convexify' the error-weight surface \cite*{lo2012,lo2017}.
This is achieved using a ``risk-averting criterion'' that should decrease the likelihood of training samples being underrepresented, but the claim is only to find better local minima as opposed to global ones.

The local minimum problems has been investigated here in St Andrews as well. 
One particularly promising approach seems to be setting subgoals on the goal path.
However, setting these subgoals requires some finesse.
\textcite{lewis1999} show that simply employing a linear chain of subgoals (such as in \textcite{gorse1997}) does not suffice in reliably finding the global minimum, but instead a non-linear chain of subgoals is required.
A technique of setting and achieving subgoals that does not rely on BP has been explored in \textcite{weir2000}.

\subsection{Implementation tools}
In both acedemia and industry, Python is the de facto standard programming language for machine learning. 
A 2019 analysis on the world-leading software development platform \href{https://www.github.com/}{GitHub} found that Python is the most popular language for open source machine learning repositories \cite{elliott2019}.
Python is a simple yet versatile language that natively supports different programming paradigms (imperative, functional, object-oriented, and more).
It is often called an interpreted language\footnote{There is nuance associated with this statement, but Python certainly exhibits more traits of an interpreted than a compiled language.} because it is dynamically typed and performs automatic memory management (garbage collection) which generally facilitates shorter code than compiled languages such as C or Java, but also means that pure-Python implementations of data-intensive algorithms will usually not be as efficient.
One of the most fundamental packages, \href{https://numpy.org/}{NumPy}, implements very efficient array manipulation operations that, although specified in Python, are carried out at a lower level for performance.

NumPy is just one piece of Python's rich ecosystem of packages that are maintained by open-source contributers in the scientific and engineering community.
The two main frameworks for machine learning are \href{https://www.tensorflow.org/}{TensorFlow} by Google and \href{https://www.pytorch.org/}{PyTorch} by Facebook. 
At their core, both frameworks facilitate the computation of mathematical operations on tensors, offering support for hardware acceleration via \textit{graphics processing units} (GPUs) and providing parallelisation strategies for distributed computing which is especially potent in the context of machine learning where many operations fit the \textit{single instruction, multple data} (SIMD) pattern.
A TensorFlow program is specified as a directed \textit{computational graph} where nodes represent operations and edges represent their inputs and outputs (data tensors) \cite{tensorflow2015whitepaper}.
In the new TensorFlow 2, this graph does not need to be explicitly constructed anymore but is created on the fly which is known as \textit{eager execution}, thereby providing the user with a simpler API similar to NumPy.
The slightly younger PyTorch framework provided dynamic computation graphs and a NumPy-like interface since its initial release in 2016, and more recently added support for static computational graphs.
Hence the newest versions of both frameworks provide similar computational capabilities.
They also facilitate the automatic computation of gradients which is useful for training neural networks. 
TensorFlow is one of the most popular repositories on GitHub, and PyTorch's popularity is rapidly growing \cite{github2019}.

\href{https://keras.io/}{Keras} is a neural network library for Python which is conceived as a high-level interface rather than a framework. 
It provides implementations of, and abstractions over, common components of neural networks such as layers, optimisers, and activation functions.
TensorFlow 2 adopted Keras as part of its core library so that the abstractions provided by Keras can easily be used with the TensorFlow backend.

One should not overlook the concept of interactive notebooks made possible by Python's interpreted nature -- that is, mixing rich text (markdown), Python code, and its output. 
Not only does this allow the programmer to make changes to the code without having to rerun the program, but it also provides a means of presenting Python code in a more engaging way than just comments.
Any type of Python output, including data visualisations, can be presented in such notebooks which makes it attractive for machine learning.
These notebooks can be created using the \href{https://jupyter.org/}{Jupyter} package or even run online in the with services such as \href{https://colab.research.google.com/}{Google Colab}. 

\section{Objectives}
The initial objectives formulated in the DOER document evolved significantly which is owed to the research-heavy nature of the project and expected.
Throughout the course of this project, as a better understanding of the theoretical aspect of neural surfing was attained by research and experimentation, the objectives were adapted.
The refined primary objectives are enumerated below in order of decreasing importance.
\begin{enumerate}
    \item Contrive a minimalist version of the stripe problem and show that it provides a strong basis for investigating and resolving the suboptimal local minimum problem for neural networks.
    \item Investigate goal-connecting paths for this problem and design a “neural surfer” that attempts to find such a goal-connecting path.
    \item Design a generic framework with a well-defined interface for implementing different gradient-based and derivative-free neural training algorithms and implement such algorithms.
    \item For this framework, implement a tool that facilitates the comparison of neural training algorithms on a given problem (dataset) by visualising arbitrary user-specified metrics (including weight and output trajectories) during training in real time.
\end{enumerate}
In addition, a secondary objective for this project was identified:
\begin{enumerate}
    \item Investigate how the neural surfer can be generalised to more complex problems.
\end{enumerate}

\section{Requirements specification}
The requirements below were formulated for the software framework aspect of the project based mainly on the third and fourth objectives from the previous section.
It is assumed that the user of the framework is familiar with Python, TensorFlow, and Keras.
\begin{enumerate}
    \item The framework should be written in Python 3 and use the TensorFlow 2 library for neural computation.
    \item The user should be able to specify a neural \textit{problem} as a Keras model with either a custom or random weight initialisation.
    \item Each problem may record \textit{metrics} during training, and the user can implement custom metrics.
    \item The user may implement a custom \textit{agent} that can be used to train on any problem specified using the framework.
    \item The user should be able to specify an \textit{experiment} that runs specified agents simultaneously, reporting and visualising the associated problem metrics for each agent in real time. 
    \item The framework should provide some implementations of agents (both gradient-based and derivative-free), problems (the stripe problem and others), and experiments for demonstrational purposes.
\end{enumerate}

\section{Software engineering process}
A very agile approach was adopted due to the primarily research-oriented character of the project.
Weekly supervisor meetings were held where progress was discussed, and tasks were set for the next week.
This ensured that changes could be made quickly depending on the outcomes of the experiments that were conducted.
The most important aspects were written up in \LaTeX{} document on a week-by-week basis\footnote{This document is available at \texttt{research/progress/main.pdf} in the code submission.} and sent to the supervisor before each meeting, so the new content could be discussed.
Apart from providing a structured set of notes for reference later on, some of the text and figures could be reused for this report.

The experiments themselves were conducted in interactive Python notebooks\footnote{The interactive notebooks are found in the \texttt{research/notebooks} folder.}, mostly using the Google Colab service to leverage free GPU acceleration.
Important statistics and results were persisted in data files\footnote{The data files are available in the \texttt{*.dat} format in the \texttt{research/data} folder.} so they could be used for analysis and plotting later.

The first version of the software framework\footnote{The software framework, as described in \ref{chap:framework} is available in the \texttt{framework} folder.} was developed over the inter-semester break and presented to the supervisor.
This version included only one agent and one problem as a proof of concept. 
Initially, visualisations were achieved using the Python library \texttt{matplotlib}, but as additional requirements unfolded (such as that the user should be able to interact with the live-updating graphs to toggle the visibility of agents), a web-based front-end was developed over the second semster using the \texttt{bokeh} library.
At the same time, more agents were implemented, and some of the experiments from the interactive notebooks were ported to the framework, too.

All code related to this project (including this report's \LaTeX{} markup itself) was maintained using the version control system Git in a single repository hosted on GitHub\footnote{The repository can be found at \href{https://github.com/georgw777/neural-surfing}{\texttt{https://github.com/georgw777/neural-surfing}}.}.
This approach was undertaken for the reason of avoiding file and code duplication: the data files produced using the interactive notebooks could be used directly to generate the plots for the weekly notes as well as this report.
Due to the fact there was only one developer, all commits were made to the \texttt{master} branch; adopting a more sophisticated model utilising feature branches or pull requests would cause more overhead than benefit in the single-developer scenario.
Furthermore, no continuous integration system with automated testing was employed because that, too, would be excessive for a research project of this scale. 

\section{Ethics}
There are no ethical considerations. 
All questions on the preliminary self-assessment form were answered with ``NO'' and hence no ethics form was completed.


