\chapter{Introduction}

This project is primarily a research project; its main focus lies in contriving and investigating an example of the local minimum problem in neural networks and designing a training regime that tackles this issue. 
Of auxiliary importance is a software framework that should be developed to not only to compare different neural training techniques, but also to ensure that the main theoretical results obtained regarding the local minimum problem are empirically verified and reproducible.

The local minimum problem, formally introduced in \ref{chap:local_minimum_problem}, 

in chapter ... we introduce a different paradigm of viewing neural training

\section{Context survey}
\subsection{Neural training}
\label{sec:context_anns}
The first mathematical model representing neurons in the human brain, so-called \textit{perceptons}, was formulated by \textcite{mcculloch1943} (see Section \ref{sec:ann}).
In 1958, the psychologist Frank Rosenblatt published the first percepton learning algorithm \cite{rosenblatt1958}, but this type of network lacked the ability to learn mappings that were not \textit{linearly separable}.
It was not until the 1980s with the introduction of the backpropagation algorithm capable of training networks with hidden layers, that neural networks experienced a substantial rise in popularity.

\paragraph{Backpropagation}
To this date, the backpropagation (BP) algorithm, attributed to \textcite{rumelhart1986}, remains the prominent method of training neural networks.
It involves computing the gradient of the loss function with respect to the weights and then using some gradient-based optimisation technique such as gradient descent to update the weights.
With the rise in popularity of deep neural networks, methods have been developed to increase the speed of converging to a minimum. 
The two main approaches are parallelising the computation and using adaptive learning rates like in the `Adam optimizer' \cite{kingma2014}.
It is well-established that BP, provided a suitable choice of hyperparameters, is guaranteed to converge to a local (but likely not global) minimum.
A common technique to subdue the effect of this issue is to run BP multiple times with different random initialisations.

\paragraph{Derivative-free optimisation}
The class of derivative-free optimisation (DFO) algorithms are optimisation techniques that attempt to find a global optimum, requiring only an objective function, but no gradient information.
One example of such an algorithm is simulated annealing (SA), proposed by \citeauthor{kirkpatrick1983}, that mimics the motion of atoms in the physical process of a slowly cooling material \cite*{kirkpatrick1983}.
Originally employed in discrete optimisation problems such as the combinatorial travelling salesman problem \cite{cerny1985}, it was later generalized and applied to problems with continuous domains \cite{belisle1993}.
However, in a comparative study of derivative-free optimisation algorithms, \citeauthor{rios2009} found that SA performed relatively poorly in comparison to more modern DFOs on general optimisation problems\footnote{It is important to note that \citeauthor{rios2009} did not assess DFOs for the purpose of neural network optimisation, but rather compared their performances on general convex and non-convex optimisation problems.} \cite*{rios2009}.

The concept of applying DFO as a means of training neural networks is not unique to this project.
In the 1990s, several training regimes for neural networks were proposed that did not rely on derivative calculations, employing variants of random and local search \cite{hirasawa1998,battiti1995}.
These approaches seemed to find better minima in some settings and did not get stuck in local minima as BP did. 
More recently, a particular random search approach was affirmed in outperforming BP in the context of deep neural networks for reinforcement learning, although a different family of DFO algorithms, so-called genetic algorithms were proposed as a superior alternative \cite{such2017}.

A very recent work presents a DFO technique for neural networks that uses a variant of local search belonging in the family of random search algorithms \cite{aly2019}.
This technique parallels the finding from other works that DFOs are often able to escape some\footnote{Guaranteed convergence to a global minimum in every scenario is not asserted, although the results indicate that the local minima are not as `poor'.} local minima and thus produce better training results; however, they require more iterations and computational resources than BP. 

\citeauthor{aly2019}, \citeauthor{such2017}, and similar works studied the performance of their respective DFO algorithms for training neural networks with a large parameter space (in the order of $10^6$ parameters) which, while providing valuable practical insight, made it infeasible to examine the structure of the loss surface analytically in order to assess issues such as severely suboptimal local minima.

\subsection{The local minimum problem}
\label{sec:context_local_minimum_problem}
The local minimum problem, which arises when an algorithm converges to a suboptimal local minimum with a comparatively high loss value, has been extensively studied as a phenomenon in optimisation problems.
However, with regards to neural networks, there seems to be differing opinions on the severity of this issue. 
One frequently cited article claims that ``In practice, poor local minima are rarely a problem with large networks'' \cite{lecun2015}.
This is underpinned in theory by other works which proved the nonexistence of suboptimal local minima, although they make varying assumptions on the structure of the underlying neural networks \cite{kawaguchi2016,nguyen2018,laurent2018}.
On the other hand, a recent article asserts that ``The apparent scarcity of poor local minima has lead practitioners to develop the intuition that bad local minima \elide are practically non-existent'' \cite{goldblum2019}.
Notwithstanding this apparent disagreement, the neural local minimum problem remains an active area of research.

\cite*{choi2008}


The local minimum problems as it relates to neural training has been investigated extensively here in St Andrews as well. 
One particularly promising approach seems to be setting subgoals on the goal path.
However, setting these subgoals requires some finesse.
\textcite{lewis1999} show that simply employing a linear chain of subgoals (such as in \textcite{gorse1997}) does not suffice in reliably finding the global minimum, but instead a non-linear chain of subgoals is required.
A technique of setting and achieving subgoals that does not rely on BP has been explored in \textcite{weir2000}.

\subsection{Implementation tools}
In both acedemia and industry, Python is the de facto standard programming language for machine learning. 
A 2019 analysis on the world-leading software development platform \href{https://www.github.com/}{GitHub} found that Python is the most popular language for open source machine learning repositories \cite{elliott2019}.
Python is a simple yet versatile language that natively supports different programming paradigms (imperative, functional, object-oriented, and more).
It is often called an interpreted language\footnote{There is nuance associated with this statement, but Python certainly exhibits more traits of an interpreted than a compiled language.} because it is dynamically typed and performs automatic memory management (garbage collection) which generally facilitates shorter code than compiled languages such as C or Java, but also means that pure-Python implementations of data-intensive algorithms will usually not be as efficient.
One of the most fundamental packages, \href{https://numpy.org/}{NumPy}, implements very efficient array manipulation operations that, although specified in Python, are carried out at a lower level for performance.

NumPy is just one piece of Python's rich ecosystem of packages that are maintained by open-source contributers in the scientific and engineering community.
The two main frameworks for machine learning are \href{https://www.tensorflow.org/}{TensorFlow} by Google and \href{https://www.pytorch.org/}{PyTorch} by Facebook. 
At their core, both frameworks facilitate the computation of mathematical operations on tensors, offering support for hardware acceleration via \textit{graphics processing units} (GPUs) and providing parallelisation strategies for distributed computing which is especially potent in the context of machine learning where many operations fit the \textit{single instruction, multple data} (SIMD) pattern.
A TensorFlow program is specified as a directed \textit{computational graph} where nodes represent operations and edges represent their inputs and outputs (data tensors) \cite{tensorflow2015whitepaper}.
In the new TensorFlow 2, this graph does not need to be explicitly constructed anymore but is created on the fly which is known as \textit{eager execution}, thereby providing the user with a simpler API similar to NumPy.
The slightly younger PyTorch framework provided dynamic computation graphs and a NumPy-like interface since its initial release in 2016, and more recently added support for static computational graphs.
Hence the newest versions of both frameworks provide similar computational capabilities.
They also facilitate the automatic computation of gradients which is useful for training neural networks. 
TensorFlow is one of the most popular repositories on GitHub, and PyTorch's popularity is rapidly growing \cite{github2019}.

\href{https://keras.io/}{Keras} is a neural network library for Python which is conceived as a high-level interface rather than a framework. 
It provides implementations of, and abstractions over, common components of neural networks such as layers, optimisers, and activation functions.
TensorFlow 2 adopted Keras as part of its core library so that the abstractions provided by Keras can easily be used with the TensorFlow backend.

One should not overlook the concept of interactive notebooks made possible by Python's interpreted nature -- that is, mixing rich text (markdown), Python code, and its output. 
Not only does this allow the programmer to make changes to the code without having to rerun the program, but it also provides a means of presenting Python code in a more engaging way than just comments.
Any type of Python output, including data visualisations, can be presented in such notebooks which makes it attractive for machine learning.
These notebooks can be created using the \href{https://jupyter.org/}{Jupyter} package or even run online in the with services such as \href{https://colab.research.google.com/}{Google Colab}. 

\section{Objectives}
During the course of this project, the initial objectives formulated in the DOER document evolved, especially as a better understanding of the theoretical aspect of neural surfing was attained.
The primary objectives are enumerated below in order of decreasing importance.
\begin{enumerate}
    \item Contrive a minimalist version of the stripe problem and show that it provides a strong basis for investigating and resolving the suboptimal local minimum problem for neural networks.
    \item Investigate goal-connecting paths for this problem and design a “neural surfer” that attempts to find such a goal-connecting path.
    \item Design a generic framework with a well-defined interface for implementing different gradient-based and derivative-free neural training algorithms and implement such algorithms.
    \item For this framework, implement a tool that facilitates the comparison of neural training algorithms on a given problem (dataset) by visualising arbitrary user-specified metrics (including weight and output trajectories) during training in real time.
\end{enumerate}
In addition, a secondary objective for this project was identified:
\begin{enumerate}
    \item Investigate how the neural surfer can be generalized to more complex problems.
\end{enumerate}

\section{Software engineering process}
\todo
\LaTeX
\section{Ethics}
There are no ethical considerations. 
All questions on the preliminary self-assessment form were answered with ``NO'' and hence no ethics form had to be completed.


