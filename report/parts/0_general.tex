\chapter{Introduction}
\textit{Describe the problem you set out to solve and the
extent of your success in solving it. You should include
the aims and objectives of the project in order of
importance and try to outline key aspects of your
project for the reader to look for in the rest of your
report.}
\todo

\section{Context survey}
\subsection{Artificial neural networks}
The first mathematical model representing neurons in the human brain, so-called \textit{perceptons}, was formulated by \textcite{mcculloch1943} (see Section \ref{sec:ann}).
In 1958, the psychologist Frank Rosenblatt published the first percepton learning algorithm \cite{rosenblatt1958}, but this type of network lacked the ability to learn mappings that were not \textit{linearly separable}.
It was not until the 1980s with the introduction of the backpropagation algorithm capable of training networks with hidden layers, that neural networks experienced a substantial rise in popularity.

\paragraph{Backpropagation}
To this date, the backpropagation (BP) algorithm, attributed to \textcite{rumelhart1986}, remains the prominent method of training neural networks.
It involves computing the gradient of the loss function with respect to the weights and then using some gradient-based optimisation technique such as gradient descent to update the weights.
With the rise in popularity of deep neural networks, methods have been developed to increase the speed of converging to a minimum. 
The two main approaches are parallelising the computation and using adaptive learning rates like in the `Adam optimizer' \cite{kingma2014}.
It is well-established that BP, provided a suitable choice of hyperparameters, is guaranteed to converge to a local (but likely not global) minimum.
A common technique to subdue the effect of this issue is to run BP multiple times with different random initialisations.

\paragraph{Derivative-free optimisation}
The class of derivative-free optimisation (DFO) algorithms are optimisation techniques that attempt to find a global optimum, requiring only an objective function, but no gradient information.
One example of such an algorithm is simulated annealing (SA), proposed by \citeauthor{kirkpatrick1983}, that mimics the motion of atoms in the physical process of a slowly cooling material \cite*{kirkpatrick1983}.
Originally employed in discrete optimisation problems such as the combinatorial travelling salesman problem \cite{cerny1985}, it was later generalized and applied to problems with continuous domains \cite{belisle1993}.
However, in a comparative study of derivative-free optimisation algorithms, \citeauthor{rios2009} found that SA performed relatively poorly in comparison to more modern DFOs on general optimisation problems\footnote{It is important to note that \citeauthor{rios2009} did not assess DFOs for the purpose of neural network optimisation, but rather compared their performances on general convex and non-convex optimisation problems.} \cite*{rios2009}.

The concept of applying DFO as a means of training neural networks is not unique to this project.
In the 1990s, several training regimes for neural networks were proposed that did not rely on derivative calculations, employing variants of random and local search \cite{hirasawa1998,battiti1995}.
These approaches seemed to find better minima and did not get stuck in local minima as BP did. 
More recently, a particular random search approach was affirmed in outperforming BP in the context of deep neural networks for reinforcement learning, although a different family of DFO algorithms, so-called genetic algorithms were proposed as a superior alternative \cite{such2017}.

A very recent work presents a DFO technique for neural networks that uses a variant of local search belonging in the family of random search algorithms \cite{aly2019}.
This technique parallels the finding from other works that DFOs are often able to escape some\footnote{Guaranteed convergence to a global minimum in every scenario is not asserted, although the results indicate that the local minima are not as `poor'.} local minima and thus produce better training results; however, they require more iterations and computational resources than BP. 

\citeauthor{aly2019}, \citeauthor{such2017}, and similar works studied the performance of their respective DFO algorithms for training neural networks with a large parameter space (in the order of $10^6$ parameters) which, while providing valuable practical insight, made it impossible to examine the structure of the loss surface analytically in order to assess issues such as severely suboptimal local minima.

\paragraph{The local minimum problem}
The local minimum problem, which arises when an algorithm converges to a suboptimal local minimum with a comparatively high loss value, has been extensively studied as a phenomenon in optimisation problems.
However, with regards to neural networks, there seems to be differing opinions on the severity of this issue. 
One frequently cited article claims that ``In practice, poor local minima are rarely a problem with large networks'' \cite{lecun2015}.
This is underpinned in theory by other works which proved the nonexistence of suboptimal local minima, although they make varying assumptions on the structure of the underlying neural networks \cite{kawaguchi2016,nguyen2018,laurent2018}.
On the other hand, a recent article asserts that ``The apparent scarcity of poor local minima has lead practitioners to develop the intuition that bad local minima \elide are practically non-existent'' \cite{goldblum2019}.
Therefore, it can be said that the local minimum problem is still an active area of research.

The local minimum problems as it relates to neural training has been investigated extensively here in St Andrews. 
One particularly promising approach seems to be setting subgoals on the goal path.
However, setting these subgoals requires some finesse.
\textcite{lewis1999} show that simply employing a linear chain of subgoals (such as in \textcite{gorse1997}) does not suffice in reliably finding the global minimum, but instead a non-linear chain of subgoals is required.
A technique of setting and achieving subgoals that does not rely on BP has been explored in \textcite{weir2000}.

\subsection{Implementation tools}
\begin{itemize}
    \item TensorFlow
    \item keras
\end{itemize}
\todo

\section{Requirements specification}
\todo

Primary objectives:
\begin{enumerate}
    \item Design a generic framework that can be used for various neural training algorithms
    with a clear set of inputs and outputs at each step. This framework should include
    benchmarking capabilities.
    \item For a simple case of this framework (when the dimensionality of the control space
    and output space are suitably low), implement a visualisation tool that shows the
    algorithmâ€™s steps.
    \item Design and implement the neural surfing technique.
    \item Evaluate the performance of this and other algorithms on tasks of differing
    complexity, especially with regard to the local minimum problem and similar issues.
\end{enumerate}
Secondary objectives:
\begin{enumerate}
    \item Investigate how this approach can be generalized to other numerical optimisation problems.
\end{enumerate}

\section{Software engineering process}
\todo

\section{Ethics}
There are no ethical considerations. 
All questions on the preliminary self-assessment form were answered with ``NO'' and hence no ethics form had to be completed.


