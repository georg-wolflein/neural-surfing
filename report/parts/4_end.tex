\chapter{Evaluation and critical appraisal}

\section{The local minimum problem}
\label{sec:eval_local_minimum_problem}
The RBF stripe problem of \ref{sec:stripe_problem} demonstrates the local minimum problem using a much simpler neural network architecture than what was analysed by \textcite{blum1989}. 
\citeauthor{blum1989} provides a proof for the existence of local minima in learning a Boolean XOR mapping\footnote{Quite interestingly, the RBF stripe problem put forth in this project can be interpreted as modelling the XOR function as well. To see this, consider \ref{fig:stripe_hyperplanes}.} using a sigmoidal neural network with two hidden units.
However, this architecture results in a six-dimensional weight space and a four-dimensional output space. 
As such, complicated mathematical tools are required to prove the existence of local minima, making it impossible to interpret the results intuitively, let alone design a training algorithm.
Moreover, there have been several publications disputing \citeauthor{blum1989}'s proof \cite{hamey1998,mizutani2010}, and even claiming that in his problem, there exists a path of strictly decreasing error from any starting weight state to the global minimum \cite{sprinkhuizen1999}.
This shows that reasoning about a problem in such abstract fashion is error-prone.

The high dimensionality of \citeauthor{blum1989}'s problem impedes effective analysis of the local minimum problem.
After all, an intuition of the problem is required in order to design an algorithm that solves it.
Therein lies the power of the RBF stripe problem because it exists in a two-dimensional weight space and three-dimensional output space.
Both spaces can be visualised intuitively, as has been demonstrated in numerous figures of this report.

The existence of two suboptimal local minima has been proven in \ref{col:stripe_suboptimal_local_minima}.
Since the error-weight surface is only three-dimensional, this proof is much simpler than the calculations required in \citeauthor{blum1989}'s proof.
In addition, \ref{fig:stripe_gradient_vector_field} provides an intuitive argument why gradient descent will converge to a local minimum under specific initial conditions, and might be more convincing than the proof itself in practice.

The methodological framework established in \ref{chap:neural_surfing_theory} facilitated a profound analysis of the RBF stripe problem, providing important insights that guided the implementation of a neural surfing technique.
It is important to appreciate that if the goal in output space is realisable, then by \ref{def:goal_connecting_path} there must exist a realisable goal-connecting path in output space from any initial configuration. 
This concept is not leveraged by classical algorithms such as BP, and as a result they are likely to get stuck in local minima.
A key observation is that the goal-connecting path, depicted in \ref{fig:stripe_ideal_goal_path}, must first lead away from the goal before eventually reaching it.
Any algorithm that does not allow this initial deviation will not be able to reach the global minimum in the RBF stripe problem.

A side-effect of the RBF stripe problem is that it provides an example of the ravine problem as well (see \ref{fig:gradient_descent_narrow_valley}).
This was observed in \ref{sec:stripe_gradient_descent} where the progress of gradient descent in the local (and global) minimum valleys was very slow.
Unlike the local minimum problem, there exist algorithms that provide solutions that are considerably more effective than BP with classical gradient descent.
One widely-used algorithm in industry, Adam, uses estimates of the mean and standard deviation of the past gradients to adapt the learning rate of weights individually \cite{kingma2014}.
Applied to the RBF stripe problem, this technique was able to converge to the local or global minimum much more quickly.
Nonetheless, Adam was found to behave worse than gradient descent with regard to other aspects such as the quality of the solution and its generalisation performance \cite{shirish2017}.
In this respect, the RBF stripe problem could be used to devise new methods of tackling the ravine problem due to its favourable qualities for analysis.

In summary, the detailed analysis of the RBF stripe problem in \ref{sec:stripe_problem} makes a strong case for the potential of this problem for investigating the local minimum problem.
Apart from providing intuition for designing algorithms that attempt to overcome this problem, it could a act as a benchmark such algorithms.
As such, it could be used to validate the claim that a novel proposed algorithm solves an instance of the local minimum problem, or at least finds near-optimal solutions -- a claim that has been made several times already as we have seen in the context survey \cite{kawaguchi2016,choi2008,hirasawa1998,lo2012,lo2017}.

\section{Neural surfing theory}
\label{sec:eval_neural_surfing}
Conventionally, neural training is treated as an optimisation problem of the error-weight surface.
As such, the BP algorithm is very popular but its sole reliance on derivative information becomes its greatest limitation with regard to the local minimum problem as was demonstrated empirically in \ref{sec:stripe_gradient_descent}.
Even DFO techniques were shown to fail in this context (see \ref{sec:local_minimum_experiments_derivative_free}).
As established in the context survey, these techniques use an element of stochasticity that allows suboptimal moves in an attempt to escape local minima.
However, it was demonstrated experimentally that this is not enough to escape the local minima of the RBF stripe problem.

The methodological framework presented in \ref{chap:neural_surfing_theory} establishes a different perspective on neural training.
In this theory, the notions of weight and output spaces as well as the concept of goal-connecting paths in both spaces are defined and analysed.
Based on the concept of ideal goal-connecting paths, the feasibility of utilising subgoals in output space has been demonstrated as an effective means of overcoming the local minimum problem.
This was shown empirically by applying the so-called `cheat'\footnote{As remarked in \ref{sec:cheating_technique}, the purpose of this technique was not to cheat in the sense of falsely claiming that the approach can solve the local minimum problem. Instead, it was developed as a means of imposing an artifical constraint so that one feature of the local minimum problem can be investigated at a time. In this case, it was used to demonstrate the feasibility of following a chain of subgoals.} technique, which was presented as part of the methodological framework in \ref{sec:cheating_technique}, to the conventional methods of training neural networks.
The finding was quite suprising: even with just one subgoal, the conventional training techniques (BP, SA, and greedy probing) were able to escape the local minimum and find the global minimum (see \ref{sec:stripe_gradient_descent_subgoals,sec:stripe_derivative_free_subgoals}).
Of course, this was only possible with a conveniently placed subgoal and an adequate learning rate that allowed the algorithms to `jump' over the top of the radial basis function, but it does show the potential for following an approach that employs subgoals.
Furthermore, as the number of subgoals along the ideal goal-connecting path was increased to around 100, the distances between these subgoals became so small that training proceeded quite smoothly.

Ultimately, the theory from chapter \ref{chap:neural_surfing_theory} as well as the previously mentioned experiments from these experiments give rise to the concept of a \textit{neural surfer}.
The core idea is that it uses information from both spaces to create intermediate subgoals to `pull' the process along.
As such, the concept of neural surfing theory is independent from its implementation.

\subsection{Implementation}
The aforementioned findings were used to develop a possible implementation of a neural surfer, taking advantage of the theory established above.
This algorithm, explained in \ref{chap:neural_surfing_technique}, attempted to generate smooth paths in output space by modelling the remaining goal-connecting path as a clothoid at each stage.
The clothoids were used not only to evaluate the quality of sample points, but also to set subgoals along the goal-connecting path.

However, this algorithm was ultimately not successful in escaping the local minima of the RBF stripe problem either.
One significant problem that was encountered is that the clothoidal paths in output space would generate illegal subgoals.
This issue was analysed, and it was proposed that an inverse sigmoidal mapping could be used to map the finite output space to an inifinite space, giving more room for the clothoids.
However, the problem of setting subgoals in unrealisable regions still remained, and thus the final implementation of the neural surfer still was not able to find the global minimum.

Nonetheless, a different version of the neural surfing algorithm that set the clothoids in excitation space instead of output space (but still trained using the corresponding subgoals in output space) was successful in finding the global minimum of the RBF stripe problem.
This simplification actually constitutes a significant limitation of this implementation because in the RBF stripe problem, the radial basis activation functions are what create the local minima.
In sigmoidal neural networks, it would be a lot less apprehensive to set the clothoids in excitation space because the sigmoid is a strictly increasing function.

However, it can still be argued that the clothoidal implementation highlights the potential of neural surfing in overcoming the stripe problem because the subgoals themselves are set in output space.
The advantage of the proposed algorithm is that unlike the other training regimes it required no manual subgoal setting.
The clothoidal approach manages to automatically set subgoals in close proximity that even enabled it to `pull' itself over the RBF hump instead of jumping over it by chance.

A drawback of the implementation is that it samples many different weight states before carrying out a move.
As commented in \ref{sec:neural_training_issues}, this is a common problem of derivative-free techniques because each forward pass is roughly as computationally expensive as evaluating the derivative at the current point (which BP only needs to do once per epoch).
In that regard, it might be possible in future to adapt this paritcular neural surfing implementation to use derivative information instead of manual sampling.
On the other hand, the fact that the clothoidal approach found the global minimum within very few iterations shows that it does not suffer from the ravine problem like gradient descent with BP.

Apart from DFO techniques that employ some element of stochasitity like SA (which was shown nonetheless to fail on the RBF stripe problem),
a common approach in practise is to perform the BP algorithm multiple times with different random initialisations in the hope that one of these lies in a global minimum basin.
In the case of the RBF stripe problem, the chance of randomly initialising at a weight configuration where BP will converge to a global minimum is 50\%.
This means that for this particular example of the local minimum problem, the random initialisation approach is feasible.
However, in larger networks, the global minimum basin may well be a lot smaller.

The advantage of the neural surfing approach in general is that it follows a more principled approach that does not rely on stochasitity but rather focuses on generating smooth trajectories in output space.
While the adaptive clothoid technique which was provided as an example of neural surfing certainly has its limitations, the underlying idea -- that is, developing an approach that uses information from both weight and output space to set subgoals -- is quite powerful.
This is highlighted in \ref{chap:generalising} which outlines several applications that a successful neural surfing implementation could have, even outwith the domain of neural networks.

\section{The neural framework}
\label{sec:eval_neural_framework}
All the requirements for the neural framework listed in \ref{sec:requirements} were satisfied, as was explained in \ref{sec:neural_framework_design} and depicted in \ref{fig:nf_components}.
The framework itself, \texttt{nf}, is quite different to popular neural network frameworks such as Keras, or more broad machine learning frameworks like TensorFlow and PyTorch, because it tries to solve an orthogonal problem.
Its target user is not a person trying to train a neural network on a dataset, but rather someone who is designing a neural training algorithm.
As such, it facilitates the comparison of custom training algorithms on set neural problem by providing real-time visualisations.

The framework is quite flexible: the user may specify custom agents, problems, metrics, and visualisations. 
Yet quite a few implementations of these components have been provided alongside the framework.
The RBF stripe problem is one of the default problem implementations, and a variety of well-known training algorithms have been implemented.
As a result, the findings from this report with regard to the failure of classical training regimes on the RBF stripe problem can be independently verified by reproducing the experiment with the framework\footnote{The experiment itself is provided in the \texttt{demo\_stripe\_problem.py} script.}. 

The supplied gradient descent agent uses the standard Keras implementation which is widely-used in industry to enable a fair comparison.
On the other hand, there exist numerous different versions of the simulated annealing algorithm in literature.
The SA implementation in the framework is a more basic version in order to keep the agent generic.
More complex implementations of SA may combine the so-called downhill simplex algorithm \cite{nelder1965} with SA such as in \textcite[p. 444-455]{press1992}, thereby introducing three additional hyperparameters.
In fact, \citeauthor{press1992} remark that ``there can be quite a lot of problem-dependent subtlety'' in choosing the hyperparameters, and that ``success or failure is quite often determined by the choice of annealing schedule'' \cite*[p. 452]{press1992}.
Since \texttt{nf} should be a generic framework, support for custom annealing schedules with features such as restarts was not implemented.
Moreover, if one were to design a custom annealing schedule which succeeds on the RBF stripe problem, that schedule will likely fail on other problems.

The framework follows best practices in Python as detailed in \ref{sec:framework_implementation}.
This also facilitated the automatic generation of an extensive documentation that is provided alongside the framework. 
Furthermore, the demonstration scripts give starter code for running custom experiments.

\section{Objectives}
This project achieved all the objectives outlined in \ref{sec:objectives}. 
The first objective was evaluated in \ref{sec:eval_local_minimum_problem} and the second in \ref{sec:eval_neural_surfing}.
The third and fourth objectives pertaining to the neural framework were reflected upon in \ref{sec:eval_neural_framework}.
Furthermore, \ref{chap:generalising} provides an overview of how the neural surfing technique could be generalised to other applications, as was the goal of the fifth objective.

\chapter{Conclusions}
The local minimum problem as it relates to neural networks is a challenging issue that has been investigated for several decades.
Its existence is acknowledged in practice which is why techniques such as multiple random initialisation are widely adopted.
Yet, this report shows that the common approaches are not satisfactory for solving the local minimum problem.

Intuitively, it is evident in the hiking analogy that set the scene in \ref{sec:motivation} and was carried through this report that there is no apparent simple solution to finding the global minimum of a mountainous landscape.
A novel example of the local minimum problem was put forth and the claim that it exhibits suboptimal local minima was proved mathematically and demonstrated empirically.
In doing so, its merits for analysing the local minimum problem were shown, especially with regard to visualising the weight space, output space, and error-weight surface requiring only two or three dimensions.
The reasons why classical training algorithms such as BP and SA fail to find the global minimum in the context of the RBF stripe problem were evaluated.

Furthermore, a methodological framework was developed that addresses the issue of neural training from a different perspective.
This `neural surfing theory' was used as a to design a candidate neural surfing algorithm.
Although that particular algorithm was not able to find the global minimum in the RBF stripe problem, it was able to do so when setting the clothoids in excitation space.
In this regard, the neural surfer was stronger than the conventional training regimes because they failed without manual subgoal setting.

Finally, a software framework was implemented for comparing neural training techniques.
Using this framework, the finding that the classical training algorithms converge to a suboptimal local minimum of the RBF stripe problem can be reproduced.
The framework facilitates a fair real-time comparison of different training algorithms on a given neural problem by visualising default and user-specified metrics.
Moreover, the framework is quite flexible in that the user may implement new custom neural problems and agents.
Best practices in Python were followed and an extensive documentation is provided.

In conclusion then, a neural training framework has been implemented to facilitate the execution, comparison, and analysis of a range of major types of training regimes.
This has been created in conjunction with analysis and design for extending problem solvability centred around a subgoal chaining technique called neural surfing.
This technique has been shown to enable smooth systematic goal-connecting paths to be formed across surface regions such as local minima and ravines that classical techniques find problematic.

\section{Future work}
\label{sec:future_work}
The RBF stripe problem was demonstrated to be a powerful yet minimal example of the local minimum problem. 
Future research into the local minimum problem could use this problem as a starting point.
Furthermore, it could serve as a benchmark for evaluating whether new proposed neural training algorithms are able to find a global minimum in the presence of severely suboptimal local minima.

One useful aspect that could be investigated further is that of unrealisable regions in output space.
In \ref{thm:stripe_unrealisable_point} it was shown that one specific point on the staight line from the initial point to the target in output space of the RBF stripe problem was strongly unrealisable.
The proof of this theorem could potentially be adapted to encompass larger regions or even to provide a means of classifying every point in output space as being either realisable or unrealisable.
This would allow the visualisation of all unrealisable regions in the three-dimensional output space and should provide some insights for devising a better neural surfing technique that is able to set the clothoids in output space instead of excitation space.

Another improvement that could be made to the neural surfer is to decrease its computational complexity by relying on derivative information instead of sample points.
Using the automatic differentiation features of popular machine learning frameworks such as TensorFlow or PyTorch, it is quite easy to find the partial derivatives of one variable with respect to another.
The unique characteristic of a neural surfer using derivative information as compared to classical BP with gradient descent will be the following:
in addition to using the derivative of the weights with respect to the loss, the neural surfer will consider the derivatives of the weights with respect to the outputs in an attempt to find a smooth goal-connecting path.

To facilitate further research into the development of a neural surfer using clothoids, an open-source mini-library for efficient clothoid construction is provided in \ref{app:clothoidlib}.
Furthermore, the neural framework can be used to evaluate the performance of candidate algorithms against the classical algorithms of BP, SA, and greedy probing.
