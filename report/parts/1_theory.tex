\chapter{Neural network theory}
\section{Supervised learning}

\begin{definition}[Regression model]
    In machine learning, a regression model $f$ is defined as a mathematical function of the form
    \begin{equation}
        \label{eq:reg_model}
        f(\vec{x}) = \hat{y} = y + \epsilon
    \end{equation}
    that models the relationship between a $D$-dimensional feature vector $\vec{x} \in \mathbb{R}^D$ of independent (\textit{input}) variables and the dependent (\textit{output}) variable $y \in \mathbb{R}$. 
    Given a particular $\vec{x}$, the model will produce a \textit{prediction} for $y$ which we denote $\hat{y}$.
    Here, the additive error term $\epsilon$ represents the discrepancy between $y$ and $\hat{y}$.
\end{definition}

\begin{definition}[Labelled dataset]
    A labelled dataset consists of $N$ tuples of the form $\langle \vec{x}_i, y_i\rangle$ for $i=1,\dots,N$.
    For each feature vector $\vec{x}_i$ (a row vector), the corresponding $y_i$ represents the observed output, or \textit{label} \cite{burkov2019}.
    We use the vector
    \begin{equation}
        \label{eq:sup_learn_target}
        \vec{y} = \begin{bmatrix}
            y_1 & y_2 & \cdots & y_N
        \end{bmatrix}\tran
    \end{equation}
    to denote all the labelled outputs in the dataset, and the $N \times D$ matrix
    \begin{equation}
        \label{eq:sup_learn_input_matrix}
        \vec{X} = \begin{bmatrix}
            \vec{x}_1 & \vec{x}_2 & \cdots & \vec{x}_N
        \end{bmatrix}\tran
    \end{equation}
    for representing the corresponding feature vectors.
\end{definition}

\paragraph{Supervised learning}
A supervised learning algorithm for a regression task infers the function $f$ given in (\ref{eq:reg_model}) from a set of \textit{labelled training data} of the form explained previously. 
We use the vector
\begin{equation}
    \label{eq:sup_learn_prediction}
    \vec{\hat{y}} = \begin{bmatrix}
        \hat{y}_1 & \hat{y}_2 & \cdots & \hat{y}_N
    \end{bmatrix}\tran
\end{equation}
to denote the prediction that $f$ produces for each training sample.

\section{Artifical neural networks}
Artifical neural networks (ANNs) take inspiration from the human brain and can be regarded as a set of interconnected neurons. 
More formally, an ANN is a directed graph of $n$ neurons (referred to as \textit{nodes} or \textit{units}) with weighted edges (\textit{links}).
Each link connecting two units $i$ and $j$ is directed and associated with a real-valued weight $w_{i,j}$. 

A particular unit $i$'s \textit{excitation}, denoted $z_i$, is calculated as the weighted sum
\begin{equation}
    z_i = \sum_{j=1}^n{w_{j,i} a_j} + b_i
\end{equation}
where $a_j \in \mathbb{R}$ is another unit $j$'s \textit{activation} and $b_i \in \mathbb{R}$ is the $i$th unit's \textit{bias}.
Notice that in this model, if there exists no link between unit $i$ and a particular $j$ then simply $w_{i,j}=0$ and therefore $j$ will not contribute to $i$'s excitation. 

The unit $i$'s activation is its excitation applied to a non-linear \textit{activation function}, $g: \mathbb{R} \rightarrow \mathbb{R}$. We have
\begin{equation}
    \label{eq:ann_activation}
    a_i = g\left(z_i\right) = g\left(\sum_{j=1}^n{w_{j,i} a_j} + b_i\right).
\end{equation}

\paragraph{Activation functions}
In its original form, \citeauthor{mcculloch1943} defined the neuron as having only binary activation \cite{mcculloch1943}. 
This means that in our model from (\ref{eq:ann_activation}), we would require $a_i \in \{0, 1\}$ and hence an activation function of the form $g_\text{thres}: \mathbb{R} \rightarrow \{0, 1\}$ which would be defined\footnote{In fact, \citeauthor{mcculloch1943} defined the activation to be zero when $x<\theta$ for a threshold parameter $\theta \in \mathbb{R}$ and one otherwise, but in our model the bias term $b_i$ acts as the threshold.} as
\begin{equation}
    \label{eq:thres_activation}
    g_\text{thres}(x) = \begin{cases} 
        0 & x < 0 \\
        1 & x \geq 0
    \end{cases}.
\end{equation}

Commonly used activation functions in modern neural networks include the sigmoid
\begin{equation}
    g_\text{sig}(x) = \frac{1}{1 + e^{-x}}
\end{equation}
and the rectified linear unit (ReLU)
\begin{equation}
    g_\text{ReLU} = \begin{cases}
        0 & x < 0 \\
        x & x \geq 0
    \end{cases}
\end{equation}
which are depicted in Figure \ref{fig:activation_functions}.
\begin{figure}
    \centering
    \begin{tikzpicture}
        \begin{axis}[
            x=0.75cm,
            y=3cm,
            axis lines=center,
            xlabel={$x$}, xlabel style={anchor=west},
            ylabel={$g_\text{sig}(x)$}, ylabel style={anchor=south},
            ymin=0, ymax=1.2,
            xmin=-3.3, xmax=3.3,
            samples=100,
            xtick={-3,...,3},
            ytick={0, 0.5, 1},
            title style={at={(0.5,1)},anchor=south,yshift=15.0},
            title={Sigmoid}
        ]
            \addplot[black] {1 / (1 + exp(-x))};
        \end{axis}
    \end{tikzpicture}
    \vspace{.05\textwidth}
    \begin{tikzpicture}
        \begin{axis}[
            x=2.25cm,
            y=3cm,
            axis lines=center,
            xlabel={$x$}, xlabel style={anchor=west},
            ylabel={$g_{\text{ReLU}}(x)$}, ylabel style={anchor=south},
            ymin=0, ymax=1.2,
            xmin=-1.1, xmax=1.1,
            samples=100,
            xtick={-1, -0.5, 0, 0.5, 1},
            ytick={0, 0.5, 1},
            title style={at={(0.5,1)},anchor=south,yshift=15.0},
            title={Rectified linear unit}
        ]
            \addplot[black][domain=0:1] {x};
            \addplot[black][domain=-1:0] {0};
        \end{axis}
    \end{tikzpicture}
    \caption{Plots of the the two most common activation functions.}
    \label{fig:activation_functions}
\end{figure}

Rectified units do not suffer from the \textit{vanishing gradient effect} \cite{glorot2011}.
This phenomenon occurs with sigmoid activation functions when they reach high saturation, i.e. when the input is significantly far from zero such that the gradient is almost horizontal.
However, the vanishing gradient problem is usually not prevelant in shallow\footnote{Shallow networks refer to ANNs with few layers.} networks so the sigmoid function still remains popular \cite{neal1992}.

Particularly in deep neural networks, different neurons (grouped in \textit{layers}, see Section \ref{sec:multi_layer_perceptron}) often have different activation functions \cite{burkov2019}, but for the purposes of this report it is more convenient (in terms of notation) to have the activation function be the same for all neurons, so it does not need to be supplied as a parameter to the function describing the particular neural network.
Much of the work in this report can easily be generalized to designs with multple activation functions.
This is because the algorithms explained in this report do not concern themselves with the specifics of the activation functions, as long as they are non-linear.

\paragraph{ANNs as regression models}
We can employ an ANN to model a regression problem of the form given in (\ref{eq:reg_model}). 
To do so, we need at least $D+1$ neurons in the network. 
We consider the first $D$ units to be the \textit{input} neurons, and the last neuron, $n$, is the output unit.
Furthermore, we require $w_{j,k}=0$ for $j,k \in \mathbb{Z}^+$ where $j \leq n$ and $k \leq D$ to ensure that there are no links feeding into the input neurons.

To obtain the prediction $\hat{y}$ given the $D$-dimensional feature vector $\vec{x}$, we set the activation of the $i$th unit to the value the $i$th element in $\vec{x}$ for $i=1,\dots,D$.
Then, we propagate the activations using (\ref{eq:ann_activation}) until finally the prediction is the activation of the last neuron, $\hat{y}=a_n$.
This process is often called \textit{forward propagation} or \textit{forward pass} \cite{russell2010}.

\subsection{Single-layer network}
We introduce a single-layer network (SLN) as a type of ANN which consists of two conceptual layers, an input and an output layer.
Every input node is connected to every output node, but there are no intra-layer links (i.e. there are no links between any two input nodes or any two output nodes), as shown in Figure \ref{fig:single_layer_perceptron}. 
This is what we call a \textit{fully-connected feedforward} architecture.
SLN architectures will always form a \textit{directed acyclic graph} (DAG) because there are no intra-layer or backwards connections.

\begin{figure}
    \begin{center}
        \begin{tikzpicture}
            [
                neuron/.style = {draw, circle, minimum size=25pt, inner sep=0pt, outer sep=0pt},
            ]
            \node [neuron] (x1) at (0,0) {$x_1$};
            \node [neuron] (x2) at (0,-2) {$x_2$};
            \node [neuron] (x3) at (0,-4) {$x_3$};
            \node [neuron] (n1) at (3,1)  {$\hat{y}_1$};
            \node [neuron] (n2) at (3,-1) {$\hat{y}_2$};
            \node [neuron] (n3) at (3,-3) {$\hat{y}_3$};
            \node [neuron] (n4) at (3,-5) {$\hat{y}_4$};
            \node (b1) at (3, 2) {$b_1$};
            \node (b2) at (3, 0) {$b_2$};
            \node (b3) at (3, -2) {$b_3$};
            \node (b4) at (3, -4) {$b_4$};
            \draw[->] (x1) -- (n1);
            \draw[->] (x1) -- (n2);
            \draw[->] (x1) -- (n3);
            \draw[->] (x1) -- (n4);
            \draw[->] (x2) -- (n1);
            \draw[->] (x2) -- (n2);
            \draw[->] (x2) -- (n3);
            \draw[->] (x2) -- (n4);
            \draw[->] (x3) -- (n1);
            \draw[->] (x3) -- (n2);
            \draw[->] (x3) -- (n3);
            \draw[->] (x3) -- (n4);
            \draw[->] (b1) -- (n1);
            \draw[->] (b2) -- (n2);
            \draw[->] (b3) -- (n3);
            \draw[->] (b4) -- (n4);
        \end{tikzpicture}
    \end{center}
    \caption{A single-layer perceptron with three input and four output neurons.}
    \label{fig:single_layer_perceptron}
\end{figure}

We purposefully use the term SLN instead of single-layer perceptron (SLP) to avoid confusion. 
A SLP has only one output unit and uses the threshold activation function given in (\ref{eq:thres_activation}) \cite{rosenblatt1958}.
In our definition of a SLN we allow more than one output and impose no restrictions on $g$, except that the same activation function is used for every output neuron.
We still use the term `single layer' because the input layer, lacking any incoming weight or bias connections, is not considered to be a `proper' layer.

Let us consider a SLN with $m$ inputs and $n$ outputs. 
Since every output unit $i$ only has connections from every input unit $j$,
we can adapt (\ref{eq:ann_activation}) to give the activation of a particular output neuron $i$ as
\begin{equation}
    a_i = y_i = g\left(z_i\right) = g\left(\sum_{j=1}^m{w_{j,i} x_j} + b_i\right)
    = g\left( \vec{w}_i\tran \vec{x}_i + b_i\right)
\end{equation}
where
$
    \vec{w}_i = \begin{bmatrix}
        w_{1,i} & w_{2,i} & \cdots & w_{m,i}
    \end{bmatrix}\tran
$
represents the weights of all the edges that connect to output unit $i$.
This is all we need to formally define a SLN.

\begin{definition}[Single-layer network]
    \label{def:sln}
    A SLN with $m$ inputs and $m$ outputs is the vector-valued function
    $\vec{S}: \mathbb{R}^m \rightarrow \mathbb{R}^n$ defined as
    \begin{equation}
        \label{eq:single_layer_perceptron}
        \vec{S}(\vec{x}; \vec{W}, \vec{b}) = \vec{g}\left( \vec{W}\tran \vec{x} + \vec{b} \right)
    \end{equation}
    where the  $m \times n$ matrix
    \begin{equation}
        \label{eq:sln_weight_matrix}
        \vec{W} = \begin{bmatrix}
            \vec{w}_1 & \vec{w}_2 & \cdots & \vec{w}_n
        \end{bmatrix} = \begin{bmatrix}
            w_{1,1} & w_{1,2} & \cdots & w_{1,n} \\
            w_{2,1} & w_{2,2} & \cdots & w_{2,n} \\
            \vdots & \vdots & \ddots & \vdots \\
            w_{m,1} & w_{m,2} & \cdots & w_{m,n}
        \end{bmatrix}
    \end{equation}
    captures all weights and the vector
    \begin{equation}
        \label{eq:sln_bias}
        \vec{b} = \begin{bmatrix}
            b_1 & b_2 & \cdots & b_n
        \end{bmatrix}\tran
    \end{equation}
    represents the biases.
    The vector-valued activation function $\vec{g} : \mathbb{R}^n \rightarrow \mathbb{R}^n$ is simply the activation function $g: \mathbb{R} \rightarrow \mathbb{R}$ applied pointwise to a vector, i.e.
    \begin{equation*}
        \vec{g}(\vec{z}) = \begin{bmatrix}
            g(z_1) & g(z_2) & \cdots & g(z_n)
        \end{bmatrix}\tran
    \end{equation*}
    for the vector of excitations
    $
        \vec{z} = \begin{bmatrix}
            z_1 & z_2 & \cdots & z_n
        \end{bmatrix}\tran
    $.
\end{definition}

Unlike the formula for a regression model, a SLN is a vector-valued function, due to the fact that there are multiple outputs. 
Note that when $n=1$, we reach the same form as in (\ref{eq:reg_model}). 
Moreover, if we additionally use the threshold activation function from (\ref{eq:thres_activation}), we arrive at the SLP model given by \citet{rosenblatt1958}.

\subsection{Multi-layer perceptron}
\label{sec:multi_layer_perceptron}
A multi-layer perceptron\footnote{Unlike SLPs, the activation function in a MLP as defined in literature does not necessarily need to be the binary threshold function $g_\text{thres}$; in fact, it is often one of the more modern activation functions explained in Section \ref{sec:activation_functions} \cite{hastie2017,burkov2019}. Hence we can use the term `multi-layer perceptron'.} (MLP) is a fully-connected feedforward ANN architecture with multiple layers which we will define in terms of multiple nested functions as in \citet{burkov2019}.
\begin{definition}[Multi-layer perceptron]
    \label{def:mlp}
    A MLP $M$ with $m$ inputs and $L$ layers is the mathematical function
    $M : \mathbb{R}^m \rightarrow \mathbb{R}$ defined as the nested function
    \begin{equation}
        M(\vec{x}; \mathscr{W}, \mathscr{B})
            = \hat{y}
            = f_L \left(
                \vec{f}_{L-1} \left(
                    \vec{\dots} \left(
                        \vec{f}_1 \left(
                            \vec{x}
                        \right)
                    \right)
                \right)
            \right)
    \end{equation}
    where $\mathscr{W} = \vec{W}_1, \vec{W}_2, \dots, \vec{W}_L$ are the weight matrices and $\mathscr{B} = \vec{b}_1, \vec{b}_2, \dots, \vec{b}_L$ the bias vectors such that the nested functions are given by $\vec{f}_l(\vec{x}) = \vec{S}(\vec{x}; \vec{W}_l, \vec{b}_l)$ for $l = 1, \dots, L-1$. 
    The outermost function $f_L$ represents a SLN with only one output unit and is hence the scalar-valued function $f_L(\vec{x}) = S(\vec{x}; \vec{W}_L, \vec{b}_L)$.
    
\end{definition}

Notice that for every $l < L$, $\vec{W}_l$ is a $n_l \times m_l$ matrix such that $n_l=m_{l+1}$ to esnure that the number of outputs of layer $l$ is the number of inputs to layer $l+1$.
This means that the MLP has $m_1$ input neurons.
Since the final layer has only one output unit, $\vec{W}_L$ has only one row, and finally $n_L=1$

The graph representing this type of network consists of connecting the outputs of the SLN representing layer $l$ with the inputs of the SLN representing layer $l+1$, as shown in Figure \ref{fig:multi_layer_perceptron}. 
The layers between the input and output layers are referred to as \textit{hidden} layers.

\begin{figure}
    \begin{center}
        \begin{tikzpicture}
            [
                neuron/.style = {draw, circle, minimum size=25pt, inner sep=0pt, outer sep=0pt},
            ]
            \node [neuron] (x1) at (0,0) {$x_1$};
            \node [neuron] (x2) at (0,-2) {$x_2$};
            \node [neuron] (x3) at (0,-4) {$x_3$};
            \node [neuron] (h11) at (3,1) {};
            \node [neuron] (h12) at (3,-1) {};
            \node [neuron] (h13) at (3,-3) {};
            \node [neuron] (h14) at (3,-5) {};
            \node [neuron] (h21) at (6,-1) {};
            \node [neuron] (h22) at (6,-3) {};
            \node [neuron] (y) at (9, -2) {$\hat{y}$};
            \node (b11) at (3, 2) {$b_{1,1}$};
            \node (b12) at (3, 0) {$b_{1,2}$};
            \node (b13) at (3, -2) {$b_{1,3}$};
            \node (b14) at (3, -4) {$b_{1,4}$};
            \node (b21) at (6, 0) {$b_{2,1}$};
            \node (b22) at (6, -2) {$b_{2,2}$};
            \node (b3) at (9, -1) {$b_{3,1}$};
            \draw[->] (x1) -- (h11);
            \draw[->] (x1) -- (h12);
            \draw[->] (x1) -- (h13);
            \draw[->] (x1) -- (h14);
            \draw[->] (x2) -- (h11);
            \draw[->] (x2) -- (h12);
            \draw[->] (x2) -- (h13);
            \draw[->] (x2) -- (h14);
            \draw[->] (x3) -- (h11);
            \draw[->] (x3) -- (h12);
            \draw[->] (x3) -- (h13);
            \draw[->] (x3) -- (h14);
            \draw[->] (h11) -- (h21);
            \draw[->] (h11) -- (h22);
            \draw[->] (h12) -- (h21);
            \draw[->] (h12) -- (h22);
            \draw[->] (h13) -- (h21);
            \draw[->] (h13) -- (h22);
            \draw[->] (h14) -- (h21);
            \draw[->] (h14) -- (h22);
            \draw[->] (b11) -- (h11);
            \draw[->] (b12) -- (h12);
            \draw[->] (b13) -- (h13);
            \draw[->] (b14) -- (h14);
            \draw[->] (b21) -- (h21);
            \draw[->] (b22) -- (h22);
            \draw[->] (b3) -- (y);
            \draw[->] (h21) -- (y);
            \draw[->] (h22) -- (y);
            \draw[dashed] (-1,3) rectangle (1,-6);
            \draw[dashed] (2,3) rectangle (7,-6);
            \draw[dashed] (8,3) rectangle (10,-6);
            \node[below] (input) at (0,3) {Input layer};
            \node[below] (hidden) at (4.5,3) {Hidden layers};
            \node[below] (output) at (9,3) {Output layer};
        \end{tikzpicture}
    \end{center}
    \caption{A multi-layer perceptron with three inputs and two hidden layers.}
    \label{fig:multi_layer_perceptron}
\end{figure}

Since MLPs are simply nested SLNs, it follows that MLPs retain the DAG property and are therefore \textit{feedforward} networks as well.
In the forward pass, the activations are propagated from layer to layer (i.e. nested function to nested function) as in (\ref{eq:single_layer_perceptron}).



\chapter{Neural network learning}
\section{Gradient descent with mean squared error}
\section{Local minimum problem}
\section{Simulated annealing}

\chapter{Neural surfing theory}
\section{Weight and output spaces}
In Definition \ref{def:mlp} we established that the tuple $\langle \mathscr{W}, \mathscr{B} \rangle$ along with the activation function is sufficient to fully define a MLP. 
Most importantly, we have $\mathscr{W} = \vec{W}_1, \vec{W}_2, \dots, \vec{W}_L$ and $\mathscr{B} = \vec{b}_1, \vec{b}_2, \dots, \vec{b}_L$ representing each layer's weight matrices and bias vectors, respectively. 

\begin{definition}[Weight space]
    \label{def:weight_space}
    The weight space $\mathcal{W}_A$ of an artificial neural network $A$ is the set of all possible assignments to its \textit{trainable parameters}. 
    The trainable parameters are its weights $\mathscr{W}$ and biases $\mathscr{B}$.
    If $A$ has $P$ trainable parameters then its weight space is defined as
    \begin{equation}
        \mathcal{W}_A = \mathbb{R}^P.
    \end{equation}
\end{definition}

\begin{definition}[Output space]
    \label{def:output_space}
    The output space $\mathcal{O}_A$ of an artificial neural network $A$ with one output neuron spans the space of all possible output predictions on the training set.
    From (\ref{eq:sup_learn_prediction}), the vector $\vec{\hat{y}}$ represents the prediction $\hat{y}$ for all $N$ training samples.
    The output space spans all possible assignments of $\vec{\hat{y}}$, so
    \begin{equation}
        \mathcal{O_A}=\mathbb{R}^N.
    \end{equation}
\end{definition}

\begin{lemma}
    \label{lmm:weight_space_sln}
    The weight space for a SLN $S$ with $m$ inputs and $n$ outputs is $\mathcal{W}_S = \mathbb{R}^{n(m+1)}$.
\end{lemma}
\begin{proof}
    $S$'s trainable parameters are the weight matrix $\vec{W} \in \mathbb{R}^{m\times n}$ from (\ref{eq:sln_weight_matrix}) and bias vector $\vec{b} \in \mathbb{R}^n$ from (\ref{eq:sln_bias}).
    By Definition \ref{def:weight_space}, the weight space encompasses all values of $\vec{W}$ and $\vec{b}$, so
    \begin{equation*}
        \mathcal{W}_S = \mathbb{R}^{m \times n} \times \mathbb{R}^{n} = \mathbb{R}^{m n + n} = \mathbb{R}^{(m + 1) n}.
    \end{equation*}
\end{proof}

\begin{lemma}
    \label{lmm:weight_space_mlp}
    A MLP $M$ with $L$ layers where the number of inputs to layer $l$ is given as $m_l$ will have the weight space $\mathcal{W}_M = \mathbb{R}^P$ where
    $P = \sum_{l=1}^{L-1}{\left(m_{l+1} (m_l + 1)\right)} + m_L + 1$.
\end{lemma}

\begin{proof}
    By Definition \ref{def:mlp}, $M$ is comprised of $L$ SLNs which we will denote $\vec{S}_1, \vec{S}_2, \dots, S_L$.
    This allows us to express the weight space of $M$ as the product of the weight spaces of each of the SLNs,
    $$
        \mathcal{W}_M
        = \prod_{l=1}^L{\mathcal{W}_{S_l}}.
    $$

    For every layer $l$, the number of inputs to $S_l$ will be the number of inputs to the $l$th layer, $m_l$. 
    Let $n_l$ denote the number outputs for each layer $l$.
    Then, by Lemma \ref{lmm:weight_space_sln},
    $$\mathcal{W}_{S_l} = \mathbb{R}^{n_l(m_l+1)}.$$
    
    By splitting of the last factor in the product of weight spaces, we obtain 
    \begin{equation*}
        \mathcal{W}_M
        = \prod_{l=1}^{L-1}{\mathcal{W}_{S_l}} \times \mathcal{W}_{S_L}
        = \prod_{l=1}^{L-1}{\mathbb{R}^{n_l(m_l+1)}} \times \mathbb{R}^{n_L(m_L+1)}.
    \end{equation*}

    Notice that for any layer $l$, the number of outputs is equal to the number of inputs to the next layer, so $n_l=m_{l+1}$ except for the last layer where there is only one output unit leaving $n_L=1$.
    This leaves
    \begin{align*}
        \mathcal{W}_M
        &= \prod_{l=1}^{L-1}{\mathbb{R}^{m_{l+1}(m_l+1)}} \times \mathbb{R}^{m_L+1} \\
        &= \mathbb{R}^{\sum_{l=1}^{L-1}{m_{l+1}(m_l+1)}} \times \mathbb{R}^{m_L+1} \\
        &= \mathbb{R}^{\sum_{l=1}^{L-1}{m_{l+1}(m_l+1)} + m_L+1},
    \end{align*}
    so $\mathcal{W}_M = \mathbb{R}^P$ with
    \begin{equation*}
        P = \sum_{l=1}^{L-1}{m_{l+1}(m_l+1)} + m_L+1.
    \end{equation*}
\end{proof}

\paragraph{Remarks}
The significance of Lemma \ref{lmm:weight_space_mlp} is that we obtain a formula for the number of trainable parameters $P$ in a MLP. 
By Definition \ref{def:weight_space}, $P$ determines the dimensionality of the weight space. 
On other other hand, Definition \ref{def:output_space} states that the number of samples in the training set $N$ determines the dimensionality of the output space.
There is no relationship between $P$ and $N$ since the number of samples in the training set can be arbitrarily chosen. 
It follows that there is no relationship between the dimensionalities of $\mathcal{W}$ and $\mathcal{O}$.

\subsection{Relationship between weight and output space}
We will now examine the nature of the mapping between the two spaces, and whether there exists a linear mapping. 
Note that linear mappings can exist between spaces of different dimensionalities \cite{rudin2006}.

\begin{definition}[Weight-output mapping]
    \label{def:weight_output_space_mapping}
    Given an artifical neural network $A:\mathbb{R}^m \rightarrow \mathbb{R}^n$ with $m$ inputs and $n$ outputs parameterized by a set of trainable parameters $\vec{w} \in \mathcal{W_A}$,
    the weight-to-output-space mapping $h_A: \mathcal{W}_A \rightarrow \mathcal{O}_A$ for a dataset with $N$ $m$-dimensional feature vectors given by the matrix
    $\vec{X} = \begin{bmatrix}
        \vec{x}_1 & \vec{x}_2 & \cdots & \vec{x}_N
    \end{bmatrix}\tran \in \mathbb{R}^{N\times m}$
    is
    \begin{equation*}
        h_A \left( \vec{w} \right)
        = \begin{bmatrix}
            A(\vec{x}_1; \vec{w} ) \\
            A(\vec{x}_2; \vec{w} ) \\
            \vdots \\
            A(\vec{x}_N; \vec{w} ) \\
        \end{bmatrix}.
    \end{equation*}

    \todo: explain confusion of the term weight-output mapping
\end{definition}

\begin{theorem}
    \label{thm:sln_one_output}
    For a SLN $S$ with one output unit, the function $h_S: \mathcal{W}_S \rightarrow \mathcal{O}_S$ is not a linear mapping.
\end{theorem}
\begin{proof}
    Let $S$ have $m$ inputs, as depicted in Figure \ref{fig:sln_m_in_1_out}.
    \begin{figure}
        \begin{center}
            \begin{tikzpicture}
                [
                    neuron/.style = {draw, circle, minimum size=25pt, inner sep=0pt, outer sep=0pt},
                ]
                
                \node [neuron] (x1)  at (0,0) {$x_1$};
                \node [neuron] (x2) at (0,-2) {$x_2$};
                \node (dots) at (0,-3) {$\vdots$};
                \node [neuron] (xm) at (0,-4) {$x_m$};
                \node [neuron] (y) at (4,-2) {$\hat{y}$};
                \node (b) at (4, -1) {$b$};
                \draw[->] (x1) -- (y) node[midway, above, sloped] {$w_1$};
                \draw[->] (x2) -- (y) node[midway, above, sloped] {$w_2$};
                \draw[->] (xm) -- (y) node[midway, above, sloped] {$w_m$};
                \draw[->] (b) -- (y);
            \end{tikzpicture}
        \end{center}
        \caption{The DAG representing a SLN with $m$ inputs and one output unit.}
        \label{fig:sln_m_in_1_out}
    \end{figure}
    Modifying the formula for a SLN given in Definition \ref{def:sln} (\ref{eq:single_layer_perceptron}) for the case where there is only one output unit, we obtain
    $\hat{y} = f_\text{SLP}(\vec{x}; \vec{w}\tran, b) = g\left(\vec{w} \vec{x} + b\right)$
    where
    $\vec{x} = \begin{bmatrix}
        x_1 & x_2 & \cdots & x_m
    \end{bmatrix}\tran$
    is the input feature vector.

    We will consider a dataset with $N$ samples where the input is given by the $N\times m$ matrix
    $\vec{X} = \begin{bmatrix}
        \vec{x}_1 & \vec{x}_2 & \cdots & \vec{x}_N
    \end{bmatrix}\tran$
    as in (\ref{eq:sup_learn_input_matrix}).
    By Definition \ref{def:weight_output_space_mapping}, the mapping from weight to output space $h_S: \mathcal{W}_S \rightarrow \mathcal{O}_S$ is
    \begin{equation*}
        h_S \left( \vec{w} \right)
        = \begin{bmatrix}
            g \left( \vec{w}\tran \vec{x}_1 + b \right) \\
            g \left( \vec{w}\tran \vec{x}_2 + b \right) \\
            \vdots \\
            g \left( \vec{w}\tran \vec{x}_N + b \right) \\
        \end{bmatrix}.
    \end{equation*}

    We will assume, by way of contradiction, that $h$ is a linear mapping. 
    From the definition of linear mappings, it must be true that $h(\vec{u} + \vec{v}) = h(\vec{u}) + h(\vec{v})$ for $\vec{u}, \vec{v} \in \mathcal{W}_S$ \cite{rudin2006}.
    On the LHS we have
    \begin{equation*}
        h \left( \vec{u} + \vec{v} \right)
        = \begin{bmatrix}
            g((\vec{u} + \vec{v})\tran \vec{x}_1 + b) \\
            g((\vec{u} + \vec{v})\tran \vec{x}_2 + b) \\
            \vdots \\
            g((\vec{u} + \vec{v})\tran \vec{x}_N + b) \\
        \end{bmatrix}
        = \begin{bmatrix}
            g(\vec{u}\tran \vec{x}_1 + \vec{v}\tran \vec{x}_1 + b) \\
            g(\vec{u}\tran \vec{x}_2 + \vec{v}\tran \vec{x}_2 + b) \\
            \vdots \\
            g(\vec{u}\tran \vec{x}_N + \vec{v}\tran \vec{x}_N + b) \\
        \end{bmatrix}
    \end{equation*}
    and on the RHS we get
    \begin{equation*}
        h \left( \vec{u} \right) + h \left( \vec{v} \right)
        = \begin{bmatrix}
            g(\vec{u}\tran \vec{x}_1 + b) \\
            g(\vec{u}\tran \vec{x}_2 + b) \\
            \vdots \\
            g(\vec{u}\tran \vec{x}_N + b) \\
        \end{bmatrix}
        + \begin{bmatrix}
            g(\vec{v}\tran \vec{x}_1 + b) \\
            g(\vec{v}\tran \vec{x}_2 + b) \\
            \vdots \\
            g(\vec{v}\tran \vec{x}_N + b) \\
        \end{bmatrix}
        = \begin{bmatrix}
            g(\vec{u}\tran \vec{x}_1 + b) + g(\vec{v}\tran \vec{x}_1 + b) \\
            g(\vec{u}\tran \vec{x}_2 + b) + g(\vec{v}\tran \vec{x}_2 + b) \\
            \vdots \\
            g(\vec{u}\tran \vec{x}_N + b) + g(\vec{v}\tran \vec{x}_N + b) \\
        \end{bmatrix}.
    \end{equation*}

    We obtain
    \begin{equation*}
        \begin{bmatrix}
            g(\vec{u}\tran \vec{x}_1 + \vec{v}\tran \vec{x}_1 + b) \\
            g(\vec{u}\tran \vec{x}_2 + \vec{v}\tran \vec{x}_2 + b) \\
            \vdots \\
            g(\vec{u}\tran \vec{x}_N + \vec{v}\tran \vec{x}_N + b) \\
        \end{bmatrix}
        = \begin{bmatrix}
            g(\vec{u}\tran \vec{x}_1 + b) + g(\vec{v}\tran \vec{x}_1 + b) \\
            g(\vec{u}\tran \vec{x}_2 + b) + g(\vec{v}\tran \vec{x}_2 + b) \\
            \vdots \\
            g(\vec{u}\tran \vec{x}_N + b) + g(\vec{v}\tran \vec{x}_N + b) \\
        \end{bmatrix}.
    \end{equation*}

    Let $\alpha_i = \vec{u}\tran \vec{x}_i$ and $\beta_i = \vec{v}\tran \vec{x}_i$ for all $i$.
    Since $\vec{u},\vec{v} \in \mathbb{R}^m$ and all $\vec{x}_i \in \mathbb{R}^m$, it follows that $\alpha_i, \beta_i \in \mathbb{R}$ for all $i$.
    Hence $g(\alpha + \beta + b) = g(\alpha + b) + g(\beta + b)$.

    The only functions that satisfy $g$ are functions that satisfy Cauchy's functional equation\footnote{Cauchy's functional equation is $f(a+b)=f(a)+f(b)$. For $a,b \in \mathbb{Q}$, the only solutions are linear functions of the form $f(x) = cx$ for some $c \in \mathbb{Q}$ \cite{reem2017}.}, but these solutions only apply when $b=0$ and furthermore are linear, whereas the activation function $g$ is non-linear. 
    We arrived at a contradiction, thus disproving our initial assumption that $h$ is a linear mapping, so it must be a non-linear mapping.
\end{proof}

\begin{corollary}
    \label{col:weight_output_sln}
    For any SLN $S$, the function $h_S: \mathcal{W}_S \rightarrow \mathcal{O}_S$ is not a linear mapping.
\end{corollary}
\begin{proof}
    We will generalize the results from Theorem \ref{thm:sln_one_output} to SLNs with multiple outputs.
    Let $S$ have $m$ inputs and $n$ outputs.
    We construct $n$ smaller SLNs, $S_1,S_2,\dots,S_n$ where each $S_i$ has all $m$ input units, but only the $i$th output unit. 
    The DAG representing $S_i$ will only contain links from the input nodes to output node $\hat{y}_i$ (and, of course, the associated bias term $b_i$) as depicted in Figure \ref{fig:sln_m_in_n_out_construction}.
    \begin{figure}
        \begin{center}
            \begin{tikzpicture}
                [
                    neuron/.style = {draw, circle, minimum size=25pt, inner sep=0pt, outer sep=0pt},
                ]
                
                \node [neuron] (x1)  at (0,0) {$x_1$};
                \node [neuron] (x2) at (0,-2) {$x_2$};
                \node (dots) at (0,-3) {$\vdots$};
                \node [neuron] (xm) at (0,-4) {$x_m$};
                \node [neuron,red] (y1) at (4,1) {$\hat{y}_1$};
                \node [neuron,green] (y2) at (4,-2) {$\hat{y}_2$};
                \node [neuron,blue] (yn) at (4,-5) {$\hat{y}_n$};
                \node [red] (b1) at (4, 2) {$b_1$};
                \node [green] (b2) at (4, -1) {$b_2$};
                \node [blue] (bn) at (4, -4) {$b_n$};
                \node (dots) at (4,-3.25) {$\vdots$};
                \draw[->,red] (x1) -- (y1);
                \draw[->,red] (x2) -- (y1);
                \draw[->,red] (xm) -- (y1);
                \draw[->,red] (b1) -- (y1);
                \draw[->,green] (x1) -- (y2);
                \draw[->,green] (x2) -- (y2);
                \draw[->,green] (xm) -- (y2);
                \draw[->,green] (b2) -- (y2);
                \draw[->,blue] (x1) -- (yn);
                \draw[->,blue] (x2) -- (yn);
                \draw[->,blue] (xm) -- (yn);
                \draw[->,blue] (bn) -- (yn);
            \end{tikzpicture}
        \end{center}
        \caption{The DAGs representing the constructions of $n$ SLNs with one output from a SLN with $m$ inputs and $n$ outputs. Each color represents one of the constructed smaller SLNs.}
        \label{fig:sln_m_in_n_out_construction}
    \end{figure}
    
    Now, we can simulate the function of $S$ by the construction
    \begin{equation*}
        S(\vec{x})
        = \begin{bmatrix}
            \hat{y}_1 \\
            \hat{y}_2 \\
            \vdots \\
            \hat{y}_n \\
        \end{bmatrix}
        = \begin{bmatrix}
            S_1(\vec{x}) \\
            S_2(\vec{x}) \\
            \vdots \\
            S_n(\vec{x}) \\
        \end{bmatrix}.
    \end{equation*}
    By Theorem \ref{thm:sln_one_output}, each $S_i$ does not have a linear mapping from weight space to output space, so $S$ cannot have a linear mapping either.
\end{proof}

\begin{corollary}[Weight-output mapping in general]
    \label{col:weight_output_mlp}
    For any MLP $M$, $h_M: \mathcal{W}_M \rightarrow \mathcal{O}_M$ is not a linear mapping.
\end{corollary}
\begin{proof}
    Let $M$ have $L$ layers.
    By Definition \ref{def:mlp}, $M$ is a nested function of $L$ SLNs. 
    Corollary \ref{col:weight_output_sln} states that each of these SLNs does not have a linear mapping from weight to output space.
    Hence the composition of $L$ SLNs that forms $M$ does not have a linear mapping from weight to output space.
\end{proof}

\paragraph{Remarks}
The findings from Corollary \ref{col:weight_output_mlp} are very significant.
They show that there is no apparent relationship between weight and output space that we can easily determine analytically. 
If there were a straightforward mapping between weight and output space, we would be able to simply be able to determine the ideal weight configuration that would achieve our target $\vec{y}$ in output space. 

However, since this is not possible, the findings above set the scene for the neural surfing technique. 
One of the core assumptions is that at a small enough scale, the mapping between weight and output space is \textit{locally linear}, or at least close enough.

\subsection{Gradient descent from the perspective of weight and output space}
\todo: Write about how SGD with MSE usually gets viewed from the perspective of error-weight space. However, it is interesting to look at the perspective of weight and output space. It becomes apparent that MSE can be thought of as greedily trying to reduce the Euclidean distance from $\vec{\hat{y}}$ to $\vec{y}$ in output space.

\section{Unrealizable regions}
\begin{definition}[Strongly unrealizable point]
    \label{def:unrealizable_point}
    Given an artificial neural network $A$, a point $\vec{p} \in \mathcal{O}_A$ in output space is \textit{strongly unrealizable} if and only if there exists no weight configuration $\vec{w} \in \mathcal{W}_A$ such that $h_A(\vec{w}) = \vec{p}$.
    In order words, it is impossible to attain $\vec{p}$.
\end{definition}
\begin{definition}[Strongly unrealizable region]
    \label{def:unrealizable_region}
    Given an artificial neural network $A$, a \textit{strongly} unrealizable region $\mathcal{U} \subset \mathcal{O}_A$ is a subspace of the output space where every point $\vec{p} \in \mathcal{U}$ is strongly unrealizable.
    
    It is apparent that there exists no neural learning algorithm that can elicit a change in weight space that will attain a point in a strongly unrealizable region in output space.
    Hence we define a \textit{weakly} unrealizable region for a particular neural learning algorithm as a region in output space that cannot be attained by a particular algorithm.
\end{definition}

\begin{lemma}
    A strongly unrealizable region cannot encompass the whole output space.
\end{lemma}
\begin{proof}
    Let us consider an artifical neural network $A$. 
    We will show that for every unrealizable region, $\mathcal{U} \subsetneq \mathcal{O}_A$.
    By Definition \ref{def:unrealizable_region}, $\mathcal{U} \subset \mathcal{O}_A$, so it remains to prove that every unrealizable region $\mathcal{U} \neq \mathcal{O}_A$.

    Choose any weight configuration $\vec{w} \in \mathcal{W}_A$. 
    Let the point $\vec{p}=h_A(\vec{w})$. 
    We know that $\vec{p}$ is \textit{not} strongly unrealizable because $\vec{w}$ achieves $\vec{p}$. 
    Hence no unrealizable region can contain $\vec{p}$, so $\vec{p} \notin \mathcal{U}$ but $\vec{p} \in \mathcal{O}_A$. 
    It follows that $\mathcal{U} \neq \mathcal{O}_A$.
\end{proof}

Let us look at a couple of examples of unrealizable regions.
\begin{figure}
    \begin{center}
        \begin{tikzpicture}
            [
                neuron/.style = {draw, circle, minimum size=25pt, inner sep=0pt, outer sep=0pt},
            ]
            \node [neuron] (x1) at (0,0) {$x_1$};
            \node [neuron] (x2) at (0,-2) {$x_2$};
            \node [neuron] (y) at (3,-1)  {$\hat{y}$};
            \node (b) at (3, 0) {$b$};
            \draw[->] (x1) -- (y);
            \draw[->] (x2) -- (y);
            \draw[->] (b) -- (y);
            \draw[->] (b) -- (y);
        \end{tikzpicture}
    \end{center}
    \caption{A simple MLP with one layer and two inputs (equivalently, a SLN with two inputs and one ouput).}
    \label{fig:sln_2_in_1_out}
\end{figure}
\begin{example}
    A trivial example of an unrealizable region is predicting two different outputs for the same training sample.
    Consider a MLP $M$ with one layer and two inputs, as shown in Figure \ref{fig:sln_2_in_1_out}.
    Let $\vec{x}\in \mathbb{R}^2$ be any point in input space.
    For this example, let the training data be the matrix
    $
        \vec{X} = \begin{bmatrix}
            \vec{x} & \vec{x}
        \end{bmatrix}\tran
    $.
    Now we can define an unrealizable region
    \begin{equation*}
        \mathcal{U} = \left\{
            \begin{bmatrix}
                p_1 \\ p_2
            \end{bmatrix} : p_1, p_2 \in \mathbb{R}, p_1 \neq p_2
        \right\}
    \end{equation*}
    because $h_M(\vec{x})$ cannot produce two different outputs for the same value of $\vec{x}$.
\end{example}
\begin{example}
    \todo: XOR function for network with one layer creates an unrealizable region
\end{example}





\todo: What if $\vec{y}$ is unrealizable?

\section{Goal-connecting paths}

\chapter{Problems}
\section{Stripe problem}


\chapter{Generalising neural surfing}
Generalize to classification as regression with multiple output variables
