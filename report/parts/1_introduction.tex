\chapter{Introduction}

This project is primarily a research-oriented project. 
That is, it is a project investigating how research on the neural manifestation of a classical problem in numerical optimisation and heuristic search may be set up and progressed.
More specifically, its main focus lies in contriving and investigating an example of the local minimum problem in neural networks\footnote{Throughout this report, the terms \textit{neural network} and \textit{artifical neural network} (ANN) are used interchangeably.} and designing a training regime that tackles this issue. 
Of auxiliary importance is a software framework that should be developed to not only to compare different neural training techniques, but also to ensure that the main theoretical results obtained regarding the local minimum problem are empirically verified and reproducible.

\section{Motivation}
\label{sec:motivation}
Imagine you are hiking up a mountain.
The trail in front of you leads gradually upwards, and in the distance, you can spot a peak.
After hiking for a while, you reach the top that peak, but then you realise it was not in fact the summit because now (from this new vantage point) you can see a small basin in front of you that leads to an even higher peak. 
This process repeats until at some point the peak up ahead will have the summit cross -- to your great relief. 
Then it is just a matter of hiking across that final valley and up to the summit.

You could only ever see the next peak because it obstructed the view of subsequent peaks until you climbed it.
This is an example of \textit{local information}: without prior knowledge or other aid there is no way of knowing if the peak in front is the penultimate one.
In fact, in the absence of trails and signposts, how would you know which way to go, if you cannot see the summit cross?

Let us continue with this thought experiment. 
Assume you could only see a distance of one metre around you, and that is the only local information available to you.
How would you find the summit?
The most natural approach is just repeatedly taking a step in the direction of the steepest upwards slope.
This method is called \textit{gradient ascent} (the gradient is just the slope).

At some point, you will reach the highest point in the one-metre vicinity which means that you are at the top of a peak. 
We call this a \textit{local optimum} because all points in the local neighbourhood are of lower elevation.
If you are standing at the summit cross, then you are very lucky and have found the highest point of the mountain.
In this case, your position marks the \textit{global optimum} because you are higher than all the other peaks this mountain has.

How do you proceed once you are at a local optimum?
When you have already reached it, your only option is to randomly move in any direction because you have no useful information.
A more principled approach might be to throughout the process sometimes allow steps that go downwards in hopes of increasing the likelihood of finding a higher peak.
This is essentially the idea behind \textit{simulated annealing} and related techniques.
However, while this technique might find a better local optimum, it is still quite unlikely to find the global optimum.
This is what we call the \textit{local optimum problem}.

Another point to consider is that if the summit did not have a cross, it would be impossible to know whether a given peak is actually the global optimum using only the local information.

To relate this thought experiment to numerical optimisation problems, let us turn it upside down: instead of trying to locate the highest peak, the goal is now to find the deepest valley, i.e. the lowest point in the landscape. 
The same logic still applies, except that not we think of an optimum as being a \textit{minimum} instead of a \textit{maximum}, and the technique is called \textit{gradient descent}.
The reason for applying this transformation is that aim of a numerical optimisation problem is to minimise a \textit{cost function} for a set of parameters. 
The surface of this function is like a mountainous landscape, and in finding its global minimum, we run into the aforementioned problem of local optima which we in this context is called the \textit{local minimum problem}.

This project will develop a technique that differs from gradient descent and simulated annealing (and other probabilistic techniques) by using a chain of intermediate targets (subgoals) along the cost surface to pull the process forwards analogous to a \textit{surfer} using a travelling wave to drive their motion. 

This report will begin with a context survey identifying not only the classical neural training algorithms, but also the efforts that have been made to deal with the local minimum problem and their limitations.
The classical theory of neural networks is presented from the ground up in \ref{chap:neural_network_theory} which naturally leads to the analysis of popular neural training techniques in \ref{chap:neural_training}.
In \ref{chap:neural_surfing_theory}, a different paradigm of viewing neural training -- `neural surfing theory' -- is introduced.
\ref{chap:local_minimum_problem} provides a precise definition of the local minimum problem, thereafter facilitating the design of an instance of said problem which is then analysed from both viewpoints described in \ref{chap:neural_network_theory,chap:neural_surfing_theory}.
The subsequent chapter lays out the neural surfing technique for undertaking the local minimum problem.
Finally, an outlook is provided how this type of technique can be generalised to other problems before finally presenting the software framework and an evaluation of the project.

\section{Context survey}
\subsection{Neural training}
\label{sec:context_anns}
The first mathematical model representing neurons in the human brain, so-called \textit{perceptrons}, was formulated by \textcite{mcculloch1943} (see \ref{sec:ann}).
In 1958, the psychologist Frank Rosenblatt published the first perceptron learning algorithm \cite{rosenblatt1958}, but this type of network lacked the ability to learn mappings that were not \textit{linearly separable}.
It was not until the 1980s with the introduction of the backpropagation algorithm capable of training networks with hidden layers, that neural networks experienced a substantial rise in popularity.

\paragraph{Backpropagation}
The backpropagation (BP) algorithm -- discovered independently by multiple researchers in the 1960s and popularised for neural networks by \textcite{rumelhart1986} -- remains the prominent method of training neural networks to this date.
It involves computing the derivative of the loss function with respect to the weights and then using some gradient-based optimisation technique (gradient descent or an approximation\footnote{Stochastic gradient descent (SGD) is often used as an approximation for gradient descent whereby the gradient is calculated on a random subset of the data (instead of the whole dataset) for computational efficiency.} thereof) to update the weights.
With the rise in popularity of deep neural networks, methods have been developed to accelerate training and allow more complex networks to feasibly learn more complex problems.
Two main approaches are parallelising the computation and using adaptive learning rates like in the `Adam optimizer' \cite{kingma2014}.
It is well-established that BP, provided a suitable choice of hyperparameters, is guaranteed to converge to a local (but likely not global) minimum.
A common technique to subdue the effect of this issue is to run BP multiple times with different random initialisations.

\paragraph{Derivative-free optimisation}
The class of derivative-free optimisation (DFO) algorithms are optimisation techniques that attempt to find a global optimum, requiring only an objective function, but no derivative information.
One example of such an algorithm is simulated annealing (SA), proposed by \citeauthor{kirkpatrick1983}, that mimics the motion of atoms in the physical process of a slowly cooling material \cite*{kirkpatrick1983}.
Originally employed in discrete optimisation problems such as the combinatorial travelling salesman problem \cite{cerny1985}, it was later generalised and applied to problems with continuous domains \cite{belisle1993}.
However, in a comparative study of derivative-free optimisation algorithms, \citeauthor{rios2009} found that SA performed relatively poorly in comparison to more modern DFOs on general optimisation problems\footnote{It is important to note that \citeauthor{rios2009} did not assess DFOs for the purpose of neural network optimisation, but rather compared their performances on general convex and non-convex optimisation problems.} \cite*{rios2009}.

The concept of applying DFO as a means of training neural networks is not unique to this project.
In the 1990s, several training regimes for neural networks were proposed that did not rely on derivative calculations, employing variants of random and local search \cite{hirasawa1998,battiti1995}.
These approaches seemed to find better minima in some settings and did not get stuck in local minima as BP did. 
More recently, a particular random search approach was affirmed in outperforming BP in the context of deep neural networks for reinforcement learning, although a different family of DFO algorithms, so-called genetic algorithms were proposed as a superior alternative \cite{such2017}.

A very recent work presents a DFO technique for neural networks that uses a variant of local search belonging in the family of random search algorithms \cite{aly2019}.
This technique parallels the finding from other works that DFOs are often able to escape some\footnote{Guaranteed convergence to a global minimum in every scenario is not asserted, although the results indicate that the local minima are not as `poor'.} local minima and thus produce better training results; however, they require more iterations and computational resources than BP. 

\citeauthor{aly2019}, \citeauthor{such2017}, and similar works studied the performance of their respective DFO algorithms for training neural networks with a large parameter space (in the order of $10^6$ parameters).
Although providing valuable practical insight, this made it infeasible to examine the structure of the loss surface analytically in order to assess issues such as severely suboptimal local minima.

A common theme underlying DFO algorithms is the promise that near-optimal solutions can be found given enough resources. 
To achieve this, they rely on stochasticity in some form or another: 
\citeauthor{kirkpatrick1983}'s SA algorithm tolerates suboptimal moves with a certain probability, and the others employ variants of random search as part of their algorithms \cite{hirasawa1998,battiti1995,aly2019,such2017}.


\subsection{The local minimum problem}
\label{sec:context_local_minimum_problem}
The local minimum problem, which arises when an algorithm converges to a suboptimal local minimum with a comparatively high loss value, has been extensively studied as a phenomenon in optimisation problems.
However, with regards to neural networks, there seems to be differing opinions on the severity of this issue. 
One frequently cited article claims that ``In practice, poor local minima are rarely a problem with large networks'' \cite{lecun2015}.
This is underpinned in theory by other works which proved the nonexistence of suboptimal local minima, although they make varying assumptions on the structure of the underlying neural networks \cite{kawaguchi2016,nguyen2018,laurent2018}.
On the other hand, a recent article asserts that ``The apparent scarcity of poor local minima has lead practitioners to develop the intuition that bad local minima \elide are practically non-existent'' \cite{goldblum2019}.
The neural local minimum problem remains an active area of research.

There have been various approaches attempting to overcome the local minimum problem as it relates to neural training.
\citeauthor{choi2008} presented a method whereby the network is split into two separate parts that are trained separately, but this technique works only on networks with one hidden layer \cite*{choi2008}.
\citeauthor{lo2012} followed a different approach through which the mean squared error function is modified in order to `convexify' the error-weight surface \cite*{lo2012,lo2017}.
This is achieved using a ``risk-averting criterion'' that should decrease the likelihood of training samples being underrepresented, but the claim is only to find better local minima as opposed to global ones.

The local minimum problems has been investigated here in St Andrews as well. 
One particularly promising approach seems to be setting subgoals on the goal path.
However, setting these subgoals requires some finesse.
\textcite{lewis1999} show that simply employing a linear chain of subgoals (such as in \textcite{gorse1997}) does not suffice in reliably finding the global minimum, but instead a non-linear chain of subgoals is required.
A technique of setting and achieving subgoals that does not rely on BP has been explored in \textcite{weir2000}.

\subsection{Implementation tools}
This project will adopt Python as the main programming language.
In both acedemia and industry, Python is the de facto standard programming language for machine learning. 
A 2019 analysis on the world-leading software development platform \href{https://www.github.com/}{GitHub} found that Python is the most popular language for open source machine learning repositories \cite{elliott2019}.
Python is a simple yet versatile language that natively supports different programming paradigms (imperative, functional, object-oriented, and more).
It is often called an interpreted language\footnote{There is nuance associated with this statement, but Python certainly exhibits more traits of an interpreted than a compiled language.} because it is dynamically typed and performs automatic memory management (garbage collection) which generally facilitates shorter code than compiled languages such as C or Java, but also means that pure-Python implementations of data-intensive algorithms will usually not be as efficient.
One of the most fundamental packages, \href{https://numpy.org/}{NumPy}, implements very efficient array manipulation operations that, although specified in Python, are carried out at a lower level for performance.

NumPy is just one piece of Python's rich ecosystem of packages that are maintained by open-source contributers in the scientific and engineering community.
The two main frameworks for machine learning are \href{https://www.tensorflow.org/}{TensorFlow} by Google and \href{https://www.pytorch.org/}{PyTorch} by Facebook. 
At their core, both frameworks facilitate the computation of mathematical operations on tensors, offering support for hardware acceleration via \textit{graphics processing units} (GPUs) and providing parallelisation strategies for distributed computing which is especially potent in the context of machine learning where many operations fit the \textit{single instruction, multple data} (SIMD) pattern.
A TensorFlow program is specified as a directed \textit{computational graph} where nodes represent operations and edges represent their inputs and outputs (data tensors) \cite{tensorflow2015whitepaper}.
In the new TensorFlow 2, this graph does not need to be explicitly constructed anymore but is created on the fly which is known as \textit{eager execution}, thereby providing the user with a simpler API similar to NumPy.
The slightly younger PyTorch framework provided dynamic computation graphs and a NumPy-like interface since its initial release in 2016, and more recently added support for static computational graphs.
Hence the newest versions of both frameworks provide similar computational capabilities.
They also facilitate the automatic computation of gradients which is useful for training neural networks. 
TensorFlow is one of the most popular repositories on GitHub, and PyTorch's popularity is rapidly growing \cite{github2019}.

\href{https://keras.io/}{Keras} is a neural network library for Python which is conceived as a high-level interface rather than a framework. 
It provides implementations of, and abstractions over, common components of neural networks such as layers, optimisers, and activation functions.
TensorFlow 2 adopted Keras as part of its core library so that the abstractions provided by Keras can easily be used with the TensorFlow backend.

One should not overlook the concept of interactive notebooks made possible by Python's interpreted nature -- that is, mixing rich text (markdown), Python code, and its output. 
Not only does this allow the programmer to make changes to the code without having to rerun the program, but it also provides a means of presenting Python code in a more engaging way than just comments.
Any type of Python output, including data visualisations, can be presented in such notebooks which makes it attractive for machine learning.
These notebooks can be created using the \href{https://jupyter.org/}{Jupyter} package or even run online in the with services such as \href{https://colab.research.google.com/}{Google Colab}. 

\section{Objectives}
The initial objectives formulated in the DOER document evolved significantly which is owed to the research-heavy nature of the project and expected.
Throughout the course of this project, as a better understanding of the theoretical aspect of neural surfing was attained by research and experimentation, the objectives were adapted.
The refined primary objectives are enumerated below in order of decreasing importance.
\begin{enumerate}
    \item Contrive a minimalist version of the stripe problem and show that it provides a strong basis for investigating and resolving the suboptimal local minimum problem for neural networks.
    \item Investigate goal-connecting paths for this problem and design a “neural surfer” that attempts to find such a goal-connecting path.
    \item Design a generic framework with a well-defined interface for implementing different gradient-based and derivative-free neural training algorithms and implement such algorithms.
    \item For this framework, implement a tool that facilitates the comparison of neural training algorithms on a given problem (dataset) by visualising arbitrary user-specified metrics (including weight and output trajectories) during training in real time.
\end{enumerate}
In addition, a secondary objective for this project was identified:
\begin{enumerate}
    \item Investigate how the neural surfer can be generalised to more complex problems.
\end{enumerate}

\section{Requirements specification}
\label{sec:requirements}
The requirements below were formulated for the software framework aspect of the project based mainly on the third and fourth objectives from the previous section.
It is assumed that the user of the framework is familiar with Python, TensorFlow, and Keras.
\begin{enumerate}
    \item The framework should be written in Python 3 and use the TensorFlow 2 library for neural computation.
    \item The user should be able to specify a neural \textit{problem} as a Keras model with either a custom or random weight initialisation.
    \item Each problem may record \textit{metrics} during training, and the user can implement custom metrics.
    \item The user may implement a custom \textit{agent} that can be used to train on any problem specified using the framework.
    \item The user should be able to specify an \textit{experiment} that runs specified agents simultaneously, reporting and visualising the associated problem metrics for each agent in real time. 
    \item The framework should provide some implementations of agents (both gradient-based and derivative-free), problems (the stripe problem and others), and experiments for demonstrational purposes.
\end{enumerate}

\section{Software engineering process}
A very agile approach was adopted due to the primarily research-oriented character of the project.
Weekly supervisor meetings were held where progress was discussed, and tasks were set for the next week.
This ensured that changes could be made quickly depending on the outcomes of the experiments that were conducted.
The most important aspects were written up in a \LaTeX{} document on a week-by-week basis\footnote{This document is available at \texttt{research/progress/main.pdf} in the code submission.} and sent to the supervisor before each meeting, so the new content could be discussed.
This facilitated a disciplined and agile approach to developing experiments and analysing their results.
Apart from providing a structured set of notes for reference later on, some of the text and figures could be reused for this report as well.

The experiments themselves were conducted in interactive Python notebooks\footnote{The interactive notebooks are found in the \texttt{research/notebooks} folder.}.
This approach is commonplace for machine learning projects, and as a side benefit this made it easy to use the notebooks in the Google Colab service to leverage free GPU acceleration.
Important statistics and results were persisted in data files\footnote{The data files are available in the \texttt{*.dat} format in the \texttt{research/data} folder.} so they could be used for analysis and plotting later.

The first version of the software framework\footnote{The software framework is available in the \texttt{framework} folder.} was developed over the inter-semester break and presented to the supervisor.
This version included only one agent and one problem as a proof of concept. 
Initially, visualisations were achieved using the Python library \texttt{matplotlib}, but as additional requirements unfolded (such as that the user should be able to interact with the live-updating graphs to toggle the visibility of agents), a web-based front-end was developed over the second semster using the \texttt{bokeh} library.
At the same time, more agents were implemented, and some of the experiments from the interactive notebooks were ported to the framework, too.

All code related to this project (including this report's \LaTeX{} markup itself) was maintained using the version control system Git in a single repository hosted on GitHub\footnote{The repository can be found at \href{https://github.com/georgw777/neural-surfing}{\texttt{https://github.com/georgw777/neural-surfing}}.}.
This approach was undertaken for the reason of avoiding file and code duplication: the data files produced using the interactive notebooks could be used directly to generate the plots for the weekly notes as well as this report.
Due to the fact there was only one developer, all commits were made to the \texttt{master} branch; adopting a more sophisticated model utilising feature branches or pull requests would cause more overhead than benefit in the single-developer scenario.
Furthermore, no continuous integration system with automated testing was employed because that, too, would be excessive for a research project of this scale. 

\section{Ethics}
There are no ethical considerations. 
All questions on the preliminary self-assessment form were answered with ``NO'' and hence no ethics form was completed.


